---
title: "Experiment 1: RDK Direction Judgement, 4 Tasks, No Error Penalty"
author: "knowlabUnimelb"
date: "2020-10-29"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

Daniel R. Little^1^, Ami Eidels^2^, and Deborah J. Lin^1^


^1^ The University of Melbourne, ^2^ The University of Newcastle

```{r load_modules, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
library(tidyverse)
library(workflowr)
#library(Rankcluster)
#library(rankdist) # Crashes rStudio
#library(Kendall)
library(DescTools) # ConDisPairs looks useful
library(gtools) 
library(english)
library(ggplot2)
library(dplyr)
#library(hrbrthemes)
library(knitr)
library(reshape2)
library(png)
library(grid)
library(lme4)
library(lmerTest)
library(rstatix)
library(jpeg)
library(pmr)
library(jmv)
library(betareg)
library(statmod)
```

```{r load_data, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
rm(list=ls())

# Load scheduling analysis functions
#  This file contains functions which are useful across all scheduling experiments
source(paste("analysis", "scheduling_analysis_functions.R", sep="/"))
#source(paste("analysis", "additional_scripts.R", sep="/"))

# Get date and format to YYMMDD
date <- getTodaysDate()

# Set the name of the experiment
experimentName = "exp1_labelled_nodelay" # use the index.Rmd file to track which experiment is which

# Name of the data file containing raw data
datafilename = "2020_exp1_rdk_data.csv" # Name of data file
inputdir <- "data" # location of raw data
datafn  = paste(inputdir, datafilename,sep="/") # Full data filename

# Read data dictionary to get column names
dataDictionaryfn = "data_dictionary.csv";
colfile = read.csv(paste(inputdir, dataDictionaryfn, sep="/"), stringsAsFactors = FALSE) # specifies the format of the columns

# Location to save selection file
selectiondir <- "selections"
selectionfn = paste(str_split(datafilename, '.csv', simplify=TRUE)[1], '_selections.csv', sep='');
selectionOutputFile <- paste(inputdir, selectiondir, selectionfn, sep="/")

# Location to save stats analysis files
outputfolder = paste(".", "analysis", "anovaData", sep = '/')  # Output folder

# Read the rawdata
rawdata = read.csv(datafn, header = FALSE, col.names = colfile$Column, colClasses = colfile$Type, stringsAsFactors = FALSE, na.strings='NA') # Add column labels to data

# Remove handful of subjects who partially completed the experiment without an id, 
rawdata = rawdata[!is.na(rawdata$unique_id), ]

# Summary data
loggedSubjects = rawdata %>% distinct(condition, unique_id)
nLoggedSubjects = rawdata %>% distinct(condition, unique_id) %>% count(condition)

```

```{r data_cleaning, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}

dataCleaningOutput = cleanRDKdata(rawdata)
cleandata <- dataCleaningOutput$cleandata
subjects <- dataCleaningOutput$subjects
nSubjects <- dataCleaningOutput$nSubjects

# Note:
# phase labels are:
# displayed_length_list (this is the exploratory phase where they click on projects)
# practice_selection - selection phase; practice - typing the selected list
# long_deadline_selection - long deadline phase (x 10); untimed - typing the list
# short_deadline_selection (x 30); deadline - typing the list

```

```{r add_phase_col, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
  cleandata = add_phase_col(cleandata, subjects$unique_id)
  cleandata$phase = as.factor(cleandata$phase)
  levels(cleandata$phase) = c("untimed", "deadline")
```

```{r demographics, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
# Need to extract emails for sending results from all datasets
# Turned off due to ethics

# Demographics
demographics = getDemographics(rawdata, nSubjects, subjects)
```

# Method

## Participants

We tested `r sum(nSubjects$n)` participants (`r demographics$numFemales` F, `r demographics$numMales` M, `r demographics$numUnspecified` Undeclared). Participants were recruited through the Melbourne School of Psychological Sciences Research Experience Pool (Mean age = `r demographics$meanAge`, range = `r demographics$minAge ` - `r demographics$maxAge`). Participants were reimbursed with credit toward completion of a first-year psychology subject. 

`r paste(toupper(substring(english(nSubjects$n[nSubjects$condition == "fixed_location"]), 1, 1)), substring(english(nSubjects$n[nSubjects$condition == "fixed_location"]), 2), sep="")` were assigned to the _Fixed Difficulty_ condition. In this condition, the location of easy, medium, hard, and very hard random dot kinematograms (RDK's) was held constant across trials. 

`r paste(toupper(substring(english(nSubjects$n[nSubjects$condition == "random_location"]), 1, 1)), substring(english(nSubjects$n[nSubjects$condition == "random_location"]), 2), sep="")` were assigned to the _Random Difficulty_ condition. In this condition, the location of easy, medium, hard, and very hard random dot kinematograms (RDK's) were randomized from trial to trial. 

The Fixed Difficulty experiment was completed before the Random Difficulty experiment. Participants only completed one of these.


## Design

```{r getCoherenceSet, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
# Get all coherence options
# .8 = Easy, .5 = Medium, .2 = Hard, .0 = Very hard
coherence_set = sort(unique(c(cleandata$patch_0, cleandata$patch_1, cleandata$patch_2, cleandata$patch_3)), decreasing = TRUE)

```

In each condition, participants completed multiple trials in which they selected and completed RDK tasks. On each trial, participants were shown a set of four RDKs labelled "Easy", "Medium", "Hard", and "Very Hard". The labels corresponded to the coherence of the RDK; that is, the proportion of dots moving in a coherent direction, which was set to 80%, 50%, 20%, and 0% for the Easy, Medium, Hard, and Very Hard locations, respectively. From the set of four RDKs, participants selected and completed one RDK at a time in any order. The goal of each trial was to complete as many as possible before a deadline. If an incorrect RDK response was made, that RDK was restarted at the same coherence but with a new randomly sampled direction, and the participant had to respond to the RDK again. A new task could not be selected until the RDK was completed successfully.

```{r taskImage, echo=FALSE, warning=FALSE, message=FALSE, results="asis"}
img <- readPNG("analysis/patch_selection.png")
grid.raster(img)
```

Participants first completed 10 trials with a long (30 sec) deadline to help participants learn the task, explore strategies, and allow for comparison to a short-deadline condition. We term this the _no deadline_ condition since the provided time is well beyond what is necessary to complete all four RDK's. Next, participants completed 30 trials with a 6 second deadline.

_Data Cleaning_

Subjects completed the experiment by clicking a link with the uniquely generated id code. Subjects were able to use the link multiple times; further, subjects were able to exit the experiment at any time. Consequently, the datafile contains partially completed data for some subjects which needed to be identified and removed. 

```{r identify_nonlearners, echo=FALSE, warning=FALSE, message=FALSE, results="asis"}
rdkoutput = removeRDKnonlearners(cleandata, dot_coherence, FALSE)
data = rdkoutput$data
rdk  = rdkoutput$rdk 
finalSubjects = rdkoutput$finalSubjects 
final_n = rdkoutput$final_n
avgTimeoutPerSubject=rdkoutput$avgTimeoutPerSubject
```

After removing any participants who had less than chance accuracy on the easiest RDK indicating equipment problems or a misunderstanding of task directions, there were `r final_n$n[final_n$condition == "fixed_location"]` and `r final_n$n[final_n$condition == "random_location"]` in the fixed and random location conditions, respectively.

```{r task_completion, echo=FALSE, message=FALSE, warning=FALSE, results="asis"}
taskStatsOutput = analyseRDKTaskCompletions(rdk, experimentName, outputfolder, printOutput = FALSE)
avgCompletions = taskStatsOutput$avgCompletions

# Uncomment to see task completion table
print(taskStatsOutput$table) 

# Uncomment to see stats output
print(taskStatsOutput$model) 

```

# Data Analysis

We first summarize performance by answering the following questions: 

## Task completions

* How many tasks are completed on average?

Across both conditions, participants completed `r round(mean(avgCompletions$mean[avgCompletions$phase == "untimed"]),2)` tasks during the `r as.character(avgCompletions$phase[1])` phase and `r round(mean(avgCompletions$mean[avgCompletions$phase == "deadline"]),2)` tasks during the `r as.character(avgCompletions$phase[2])` phase. 

As one might expect, there were fewer tasks completed under a deadline than without a deadline. There was no difference between conditions and no interaction between deadline and location condition.

## RDK performance

```{r rdk_anova, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
rdkANOVAoutput = analyseRDKdata(rdk)
rdkANOVAdata = rdkANOVAoutput$anovaData
rdkANOVAaccuracy = rdkANOVAoutput$accuracy
rdkANOVAsummedRT = rdkANOVAoutput$summedRT
```

We next analysed performance on the RDK discriminations. We then asked:

* What was the average completion time and accuracy of the easy, medium, hard, and very hard tasks? 

RTs became shorter and more accurate as the difficulty of the RDK became easier. As expected, the RTs were shorter under a deadline than without a deadline. We visualised the response times in two ways: First, we simply took the average of each attempt on each RDK. 

```{r difficulty_plot, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
makeRDKplots(rdkANOVAdata)
```

Second, we computed the time to complete an RDK as the cumulative sum across multiple attempts within a trial (termed Cumulative RT or cRT). That is, if an error is made and the RDK needs to be repeated, then the total RT is the sum of both attempts. 

We further broke down RTs by condition, deadline, and difficulty. 

```{r conXphaseXdiff, echo=FALSE, warning=FALSE, message=FALSE, results="asis"}
diffOutput = computeRewardRate(rdk)
kable(diffOutput$difficulty, digits = 2, caption="Mean accuracy, RT, summed RT, and reward rate attempts for each difficulty and each phase")

rrdata = diffOutput$rrdata

```

## Reward Rate

To confirm that coherence offered a good proxy for difficulty (and hence, that an optimal order of easiest to hardest was maintained), we calculated the reward rate for each patch. Reward rate can be defined as “the proportion of correct trials divided by the average duration between decisions” (Gold & Shadlen, 2002), and is tantamount, in our task, to  percentage of correct responses per unit time (Bogacz et al, 2006). For our purposes, we can fix time at 1 sec calculate the Reward Rate as the number of RDK tasks completed in 1 sec.

Inspection of the figure reveals that RR is roughly monotonically increasing when tasks become easier. Under such conditions, the optimal order of task-completion should be easy-to-hardest. This could change in a predictable manner if people value differently easy and hard tasks (overweight completion of harder tasks). The only notable exception was in the fixed, no deadline task, where the easy and medium RDK conditions had equal RR. 


```{r rewardRate_analysis, echo=FALSE, warning=FALSE, message=FALSE, results="asis"}
# Plot reward rate
plotRewardRate(rrdata)

# Reward Rate ANOVA
rrAOV = rewardRateANOVA(rrdata);
rr.aov = rrAOV$anovaData

# Within subjects
kable(rr.aov$rmTable$asDF, digits=3)

# Between subjects
kable(rr.aov$bsTable$asDF, digits=3)

```


## Optimality in each condition

Having now established that the RDK's are ordered in accuracy, difficulty, and reward rate, it is clear that the task set presented to each subject has an optimal solution, ordered from easiest to most difficult. We now ask: 

* What is the proportion of easy, medium, hard, and very hard tasks selected first, second, third or fourth? 

We first compute the marginal distribution of the ranks of each of the tasks; in other words, what are the proportions of the ranks of each task in each rank position. These matrices indicate the proportions of responses for each difficulty level which were chosen first, second, third, or fourth, respectively. The matrix from a dataset in which choice is always optimal would have ones on the diagonal and zeros on the off-diagonal.


```{r get_selections, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
numberOfTasks = 4 # Number of tasks
selections = getRDKselections(data, selectionOutputFile, numberOfTasks)

```

```{r find_heatmap, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
fu = makeHeatmap(selections, "fixed_location"  , "untimed" )
fd = makeHeatmap(selections, "fixed_location"  , "deadline" )
ru = makeHeatmap(selections, "random_location" , "untimed" )
rd = makeHeatmap(selections, "random_location" , "deadline" )
```

```{r heatmap_plot_over_subs, echo=FALSE, warning=FALSE, message=FALSE, results="asis"}
  plotHeatmap(fu$avg_rankagg, fd$avg_rankagg, ru$avg_rankagg, rd$avg_rankagg, c('fixed', 'random'))
```

```{r hierarchical_loglinear_model , echo=FALSE, warning=FALSE, message=FALSE, results="asis"}
hmOutput = heatmapStats(selections)

# Uncomment to see output
summary(hmOutput)
```

* Do the marginal distributions differ from uniformity?

We tested whether the marginal distributions were different from uniformally random selection using the fact that the mean rank is distributed according to a  $\chi^2$ distribution with the following test-statistic:
$$\chi^2 = \frac{12N}{k(k+1)}\sum_{j=1}^k \left(m_j - \frac{k+1}{2} \right)^2$$
see (Marden, 1995). 

```{r chi2uniformity, echo=FALSE, warning=FALSE, message=FALSE, results="asis"}
uniformityTest(selections, c("fixed_location", "random_location"), c("untimed", "deadline"), fu, fd, ru, rd) 
```

It is evident at a glance that the ordering of choices is more optimal when the locations are fixed; that is, the proportions on the diagonal are higher. When the locations are fixed, choice order becomes more optimal under a deadline. By contrast, when locations are random, responding becomes _less_ optimal under a deadline. This likely reflects the additional costs of having to search for the appropriate task to complete. This search is minimised in the fixed location condition. 


* How optimal were responses? 

```{r distanceAnalysis, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
# Add distances to selections data frame
selections = getDistances(selections)
plotDistances(selections)
distanceAnalysis = distanceStats(selections)
summary(distanceAnalysis)

#ksOutput = analyseDistances(selections, c("fixed_location_delay", "random_location_delay"), c("untimed", "deadline"))

# Uncomment to view output
# print(ksOutput)

```

The next analysis computed the distance between the selected order and the optimal order (easiest to very hard for that trial), which ranges between 0 (perfect match) and 6 (maximally distant), for 4 options. 

What we want is the distance of the selected options from the optimal solutions, which is the edit distance (or number of discordant pairs) between orders. However, because a participant may run out of time, there may be missing values. To handle these values, for each trial, we find the orders which partially match the selected order and compute three the average distance of those possible orders and the optimal solution (*avg_distance*). 

The following figure compares the avg_distance between the fixed difficulty and random difficulty conditions as a function of deadline condition and phase. For each of these measures, lower values reflect respones which are closer to optimal.

```{r plotData, echo=FALSE, warning=FALSE, message=FALSE, results="asis"}
# max_plot <- avg_maxdata %>% ggplot(aes(x=avg, fill=phase)) + geom_histogram(color="white", alpha=0.6, bins = 10, position="identity") + facet_wrap(~condition+phase) + labs(y="Frequency", x = "Max Distance")
# print(max_plot + ggtitle("Max Distance"))
# 
# min_plot <- avg_mindata %>% ggplot(aes(x=avg, fill=phase)) + geom_histogram(color="white", alpha=0.6, bins = 10, position="identity") + facet_wrap(~condition+phase) + labs(y="Frequency", x = "Min Distance")
# print(min_plot + ggtitle("Min Distance"))

avg_plot <- selections %>% ggplot(aes(x=avgD, fill=phase)) + geom_histogram(aes(y=..density..), color="white", alpha=0.6, bins = 6, position="identity") + facet_wrap(~condition+phase) + labs(y="Probability", x = "Avg Distance")
#print(avg_plot + ggtitle("Avg Distance"))


```
## Stability of selections
```{r entropyAnalysis, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
subEntropy = getSubjectEntropy(selections)
makeSubjectEntropyPlot(subEntropy)
entropyModel = entropyStats(subEntropy)

```

```{r entropyBlockAnalysis, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
blockEntropy = getEntropyOverBlocks(selections)
makeBlockEntropyPlot(blockEntropy)

subBlockEntropy = getSubjectEntropyOverBlocks(selections)
blockEntropyModel = lmer(mean_entropy ~ condition + block + condition:block + (1|subject), subBlockEntropy)
summary(blockEntropyModel)
```

## Selection Choice RTs

```{r first_choice_rt, echo=FALSE, warning=FALSE, message=FALSE, results="asis"}
selection_mean_rts = selections %>% 
  group_by(condition, phase) %>% 
  summarise("mrt_sel1" = mean(rt_1, na.rm=TRUE), "mrt_sel2" = mean(rt_2, na.rm=TRUE), "mrt_sel3" = mean(rt_3, na.rm=TRUE), "mrt_sel4" = mean(rt_4, na.rm=TRUE)) 

kable(selection_mean_rts, caption = "Mean RTs for each scheduling selection", digits = 2)

rtAOV = selectionRTstats(selections)
print(rtAOV)
```


# Selection model

We can treat each task selection as a probabilistic choice given by a Luce's choice rule (Luce, 1959), where each task is represented by some strength, $\nu$. The probability of selecting task $i_j$ from set $S = \{i_1, i_2, ..., i_J \}$, where J is the number of tasks, is:

$$p\left(i_j |S \right) = \frac{\nu_{i_j}}{\sum_{i \in S} \nu_{i}}. $$

Plackett (1975) generalised this model to explain the distribution over a sequence of choices (i.e., ranks). In this case, after each choice, the choice set is reduce by one (i.e., sampling without replacement). This probability of observing a specific selection order, $i_1 \succ ... \succ i_J$ is:

$$p\left(i_j |A \right) = \prod_{j=1}^J \frac{\nu_{i_j}}{\sum_{i \in A_j} \nu_{i}}, $$

where $A_j$ is the current choice set.


```{r PLmodel_bySubject, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
fu = selections %>% 
  filter(condition == "fixed_location", phase == "untimed") %>% 
  select(subject, trial, selected_difficulty_1, selected_difficulty_2, selected_difficulty_3, selected_difficulty_4) %>%
  as.matrix() %>% 
  getPLparametersBySubject()

fd = selections %>% 
  filter(condition == "fixed_location", phase == "deadline") %>% 
  select(subject, trial, selected_difficulty_1, selected_difficulty_2, selected_difficulty_3, selected_difficulty_4) %>%
  as.matrix() %>% 
  getPLparametersBySubject()

ru = selections %>% 
  filter(condition == "random_location", phase == "untimed") %>% 
  select(subject, trial, selected_difficulty_1, selected_difficulty_2, selected_difficulty_3, selected_difficulty_4) %>%
  as.matrix() %>% 
  getPLparametersBySubject()

rd = selections %>% 
  filter(condition == "random_location", phase == "deadline") %>% 
  select(subject, trial, selected_difficulty_1, selected_difficulty_2, selected_difficulty_3, selected_difficulty_4) %>%
  as.matrix() %>% 
  getPLparametersBySubject()
```

```{r PLmodel_table, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
slopemeans = c(mean(fu$plSlopes), mean(fd$plSlopes), mean(ru$plSlopes), mean(rd$plSlopes))
slopeses = c(sd(fu$plSlopes)/sqrt(length(fu$plSlopes)), sd(fd$plSlopes)/sqrt(length(fd$plSlopes)), 
             sd(ru$plSlopes)/sqrt(length(ru$plSlopes)), sd(rd$plSlopes)/sqrt(length(rd$plSlopes)))

plTable = data.frame("condition" = c("fixed", "fixed", "random", "random"),
                     "phase" = c("untimed", "deadline", "untimed", "deadline"),
                     "pl.easy"     = c(fu$mean_pl[1], fd$mean_pl[1], ru$mean_pl[1], rd$mean_pl[1]),
                     "pl.easy.se"  = c(fu$se_pl[1]  , fd$se_pl[1],   ru$se_pl[1],   rd$se_pl[1]),
                     "pl.med"      = c(fu$mean_pl[2], fd$mean_pl[2], ru$mean_pl[2], rd$mean_pl[2]),
                     "pl.med.se"   = c(fu$se_pl[2]  , fd$se_pl[2],   ru$se_pl[2],   rd$se_pl[2]),
                     "pl.hard"     = c(fu$mean_pl[3], fd$mean_pl[3], ru$mean_pl[3], rd$mean_pl[3]),
                     "pl.hard.se"  = c(fu$se_pl[3]  , fd$se_pl[3],   ru$se_pl[3],   rd$se_pl[3]),
                     "pl.vhard"    = c(fu$mean_pl[4], fd$mean_pl[4], ru$mean_pl[4], rd$mean_pl[4]),
                     "pl.vhard.se" = c(fu$se_pl[4]  , fd$se_pl[4],   ru$se_pl[4],   rd$se_pl[4]),
                     "slope"       = slopemeans,
                     "slope.se"    = slopeses)
                    
knitr::kable(plTable, caption = "Mean strength estimates (and standard errors) for Plackett-Luce model", digits=2)


```

```{r PLslope_ANOVA, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
# Combine arrays into a data frame with labels
pldata <- data.frame(
  value = c(fu$plSlopes, fd$plSlopes, ru$plSlopes, rd$plSlopes),
  condition = rep(c("fixed", "fixed", "random", "random"), 
                  times = c(length(fu$plSlopes), length(fd$plSlopes), length(ru$plSlopes), length(rd$plSlopes))),
  phase = rep(c("untimed", "deadline", "untimed", "deadline"), 
              times = c(length(fu$plSlopes), length(fd$plSlopes), length(ru$plSlopes), length(rd$plSlopes)))
)

anova_result <- aov(value ~ condition * phase, data = pldata)

# Print the summary of the ANOVA
summary(anova_result)

```

## Sampling distribution anlaysis

In order to characterise performance, we examined three sampling distributions for comparison to our data. The first is the sampling distribution of edit distances from optimal assuming that orders are sampled uniformly at random. The second distribution assumes that the first choice was optimal but the remaining orders are sampled at random. Finally, the third distributions assumes that the first two choices are selected optimally but that the remaining are randomly selected. It is clear that the mode of the distribution moves from a distance of 3 to a distance of 0 as the sampling distribution summarises orders which better conform to optimality. 

To characterise the optimality of each condition at each point in the experiment, we first computed the ks-test statistic between the data (the average partial distance data) and the random order distribution and the first-two optimal distribution. Since smaller ks-statistics indicate a closer match between the distributions, we then took the ratio of the ks-statistics (random over first two-optimal). Values less than one indicate that the data are more consistent with random than optimal responding. Values greater than one indicate that the data are more consistent with optimal rather than random responding. 

```{r sampling_dist, echo=FALSE, warning=FALSE, message=FALSE, results="asis"}

  allperms = permutations(n=4, r=4, v=1:4, repeats.allowed=FALSE)
  nperms = nrow(allperms)

opt_order_dist = function(trial_optimal_order, nperms=nperms, allperms=allperms){
  pdist <- rep(NA, nperms)
  for (i in 1:nperms){
    distObj = ConDisPairs(table(allperms[i, ], trial_optimal_order))
    pdist[i] = distObj$D  
  }
  return(pdist)
}

opt_order = c(1,2,3,4)
opt_dist_list = opt_order_dist(opt_order, nperms, allperms)

nsamples = 1000

# sample a bunch of orders, and compute the distances
allrand = as.data.frame(list("s" = sample(opt_dist_list, nsamples, replace = TRUE)))
onefix = as.data.frame(list("s" = sample(opt_dist_list[allperms[,1]==1], nsamples, replace = TRUE)))
twofix = as.data.frame(list("s" = sample(opt_dist_list[allperms[,1]==1 & allperms[,2]==2], nsamples, replace = TRUE)))
distance_samples = as.data.frame(rbind(cbind("cond" = rep(1, nsamples), "s" = allrand$s), cbind("cond" = rep(2, nsamples), "s" = onefix$s), cbind("cond" = rep(3, nsamples), "s" = twofix$s)))


# plot the distribution of distances
samp_plot <- distance_samples %>% ggplot(aes(s)) + 
  geom_histogram(color="white", alpha =0.8, bins=7, position="identity") + 
  labs(y = "Frequency of Sample", x = "Distance From Optimal") + 
  facet_wrap(~cond)
print(samp_plot)

# Statistical comparison
#ks.test(avg_avgdata$avg[avg_avgdata$condition == "fixed" & avg_avgdata$phase == "deadline early"], avg_avgdata$avg[avg_avgdata$condition == "fixed" & avg_avgdata$phase == "deadline late"])

# Lower values of the ks test indicate higher similarity between the distributions, so compare the ratio of optimal vs random distances at each time point
# Note: you don't have to sample for the kstest, you can just use the actual values, but sampling is fine
# > 1 indicates more optimal than random, < 1 indicates more random than optimal

ks.fixed.phase1 = ks.test(allrand$s, selections %>% filter(condition == "fixed_location", phase == "untimed") %>% pull(avgD))$statistic/ks.test(twofix$s, selections %>% filter(condition == "fixed_location", phase == "untimed") %>% pull(avgD))$statistic

ks.fixed.phase2 = ks.test(allrand$s, selections %>% filter(condition == "fixed_location", phase == "deadline") %>% pull(avgD))$statistic/ks.test(twofix$s, selections %>% filter(condition == "fixed_location", phase == "deadline") %>% pull(avgD))$statistic

ks.random.phase1 = ks.test(allrand$s, selections %>% filter(condition == "random_location", phase == "untimed") %>% pull(avgD))$statistic/ks.test(twofix$s, selections %>% filter(condition == "random_location", phase == "untimed") %>% pull(avgD))$statistic

ks.random.phase2 = ks.test(allrand$s, selections %>% filter(condition == "random_location", phase == "deadline") %>% pull(avgD))$statistic/ks.test(twofix$s, selections %>% filter(condition == "random_location", phase == "deadline") %>% pull(avgD))$statistic

# Build data for plotting
randomness = as.data.frame(list("condition" = factor(c("fixed", "fixed", "random", "random"), levels =c("fixed", "random")), 
                                "phase" = factor(c("untimed", "deadline", "untimed", "deadline"), levels = c("untimed", "deadline")), 
                                "ratio" = c(ks.fixed.phase1, ks.fixed.phase2, ks.random.phase1, ks.random.phase2)))
levels(randomness$phase) = c("untimed", "deadline")

# Plot
random_eval_plot <- randomness %>% ggplot(aes(x=phase, y=ratio, group=condition)) + 
                     geom_line(aes(linetype=condition), size=1) +
                     geom_point(size=4) + 
                     labs(y="Ratio (Random Distance/Optimal Distance)", x = "Phase") + 
                     ggtitle("Comparison to random and optimal sampling distributions") + 
                     geom_hline(yintercept =1, linetype="dotted", color="black", size=.5)
print(random_eval_plot)

```

This figure efficiently summarises the main result: responding is more optimal in the fixed deadline condition particularly under a deadline; in the random deadline conditions, responding was closer to a random sampling distribution than to an optimal sampling distribution. 

<!-- ## Alternative response strategies -->

<!-- An alternative possible strategy involves selecting RDKs based on spatial position. One salient strategy would be to start with a random RDK and then select the remaining tasks in clockwise and anti-clockwise order.  -->

```{r spatial_analysis, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
# 
# clockwise = rbind(c(0, 1, 2, 3), c(1, 2, 3, 0), c(2, 3, 0, 1), c(3, 0, 1, 2))
# anticlockwise = rbind(c(0, 3, 2, 1), c(1, 0, 3, 2), c(2, 1, 0, 3), c(3, 2, 1, 0))
# spatial_orders = rbind(clockwise, anticlockwise)
# 
# # Code whether each selection is consistent or inconsistent with a circular strategy
# get_spatial_consistency = function(sdata, spatial_orders, ntrials, n, nSubjects, subjects){
#   
#   # preallocate variables for output
#   spatial_choice_scores <- matrix(data=NA, nrow=nSubjects, ncol=ntrials)
#   spatial_display_scores <- matrix(data=NA, nrow=nSubjects, ncol=ntrials)
#   for (j in 1:nSubjects){
#     # select out data for subject j
#     tdata = sdata[sdata$uniqueid == subjects[j], ]
#     
#     # preallocated subject specific task selection matrix
#     #selmat <- matrix(data=NA, nrow=ntrials, ncol=n)
#     for (i in 1:ntrials){
# 
#         # Get the task selection order for the current trial (may be incomplete if timed out)
#         csel = tdata[tdata$trial_number == i-1, ]$button_pressed+1
#         #csel[csel==5] = NA # NULL is coded as option 5, Replace NULL with NA
#         #csel = as.numeric(as.character(as.matrix(csel)))
#         
#         #selmat[i, 1:length(csel)] = csel # NULL is coded as option 5
#         #selmat[i, selmat[i,] == 5] = NA  # Replace NULL with NA
#         
#         # Get coherence order for the current trial
#         coh = c(tdata[tdata$trial_number == i-1, ]$patch_0[1], tdata[tdata$trial_number == i-1, ]$patch_1[1], tdata[tdata$trial_number == i-1, ]$patch_2[1], tdata[tdata$trial_number == i-1, ]$patch_3[1])
#         
#         cohOrder = order(coh) # Locations of each difficulty (very hard, hard, med, easy)
#         
#         # Code trial and coherence as spatially consistent or not
#         spatial_choice_scores[j,i] = as.numeric(any(apply(spatial_orders[ , 1:3], 1, function(x) identical(x, csel[1:3]))))
#         spatial_display_scores[j,i] = as.numeric(any(apply(spatial_orders[ , 1:3], 1, function(x) identical(x, cohOrder[1:3]-1))))
#     }
#   }
#   return(list("choice" = spatial_choice_scores, "display" = spatial_display_scores))
# }
# 
# 
# # Compute responses consistent with a spatial strategy
# # sdata, spatial_orders, ntrials, n, nSubjects, subjects
# spatial_scores = list("fixed" = list("practice" = get_spatial_consistency(splitData(fd$data)$practice, spatial_orders, 10, 
#                                                          n, fd$nSubjects, fd$subjects), 
#                             "experiment" = get_spatial_consistency(splitData(fd$data)$experiment, spatial_orders, 30,
#                                                          n, fd$nSubjects, fd$subjects)), 
#              "random" = list("practice" = get_spatial_consistency(splitData(rd$data)$practice, spatial_orders, 10, 
#                                                          n, rd$nSubjects, rd$subjects), 
#                             "experiment" = get_spatial_consistency(splitData(rd$data)$experiment, spatial_orders, 30,
#                                                          n, rd$nSubjects, rd$subjects)))

```


```{r buildSpatialComparison, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
# fscon = data.frame(condition = as.factor(rep(1, fd$nSubjects*10)), 
#                   subjects = rep(1:fd$nSubjects + 100, each=10), 
#                   trials = rep(1:10, fd$nSubjects), 
#                   sc_practice = matrix(t(spatial_scores$fixed$practice$choice), 10*nrow(spatial_scores$fixed$practice$choice), byrow= FALSE), 
#                   sc_expFirst10 = matrix(t(spatial_scores$fixed$experiment$choice[, 1:10]), 10*nrow(spatial_scores$fixed$experiment$choice), byrow= FALSE), 
#                   sc_expLast10 = matrix(t(spatial_scores$fixed$experiment$choice[, 21:30]), 10*nrow(spatial_scores$fixed$experiment$choice), byrow= FALSE), 
#                   sd_practice = matrix(t(spatial_scores$fixed$practice$display), 10*nrow(spatial_scores$fixed$practice$display), byrow= FALSE), 
#                   sd_expFirst10 = matrix(t(spatial_scores$fixed$experiment$display[, 1:10]), 10*nrow(spatial_scores$fixed$experiment$display), byrow= FALSE), 
#                   sd_expLast10  = matrix(t(spatial_scores$fixed$experiment$display[, 21:30]), 10*nrow(spatial_scores$fixed$experiment$display), byrow= FALSE))
# 
# rscon = data.frame(condition = as.factor(rep(2, rd$nSubjects*10)), 
#                   subjects = rep(1:rd$nSubjects + 200, each=10), 
#                   trials = rep(1:10, rd$nSubjects), 
#                   sc_practice = matrix(t(spatial_scores$random$practice$choice), 10*nrow(spatial_scores$random$practice$choice), byrow= FALSE), 
#                   sc_expFirst10 = matrix(t(spatial_scores$random$experiment$choice[, 1:10]), 10*nrow(spatial_scores$random$experiment$choice), byrow= FALSE), 
#                   sc_expLast10 = matrix(t(spatial_scores$random$experiment$choice[, 21:30]), 10*nrow(spatial_scores$random$experiment$choice), byrow= FALSE), 
#                   sd_practice = matrix(t(spatial_scores$random$practice$display), 10*nrow(spatial_scores$random$practice$display), byrow= FALSE), 
#                   sd_expFirst10 = matrix(t(spatial_scores$random$experiment$display[, 1:10]), 10*nrow(spatial_scores$random$experiment$display), byrow= FALSE), 
#                   sd_expLast10  = matrix(t(spatial_scores$random$experiment$display[, 21:30]), 10*nrow(spatial_scores$random$experiment$display), byrow= FALSE))
# 
# spatialdata = rbind(fscon, rscon)
# 
# # Set up summary data dataframe: spatial choice consistency
# sc_a = setNames(aggregate(spatialdata$sc_practice, by=list(spatialdata$condition, spatialdata$subject), mean), c("condition", "subject", "avg"))
# sc_b = setNames(aggregate(spatialdata$sc_expFirst10, by=list(spatialdata$condition, spatialdata$subject), mean), c("condition", "subject", "avg"))
# sc_c = setNames(aggregate(spatialdata$sc_expLast10, by=list(spatialdata$condition, spatialdata$subject), mean), c("condition", "subject", "avg"))
# 
# avg_scdata = rbind(cbind(sc_a, phase=rep("untimed practice", nrow(sc_a))), 
#                     cbind(sc_b, phase=rep("deadline early", nrow(sc_b))), 
#                     cbind(sc_c, phase=rep("deadline late", nrow(sc_c))))
# avg_scdata$phase = as.factor(avg_scdata$phase)         # Convert phase to factor
# levels(avg_scdata$condition) = c("fixed", "random") # Replace condition names
# 
# # Set up summary data dataframe: spatial display consistency
# sd_a = setNames(aggregate(spatialdata$sd_practice, by=list(spatialdata$condition, spatialdata$subject), mean), c("condition", "subject", "avg"))
# sd_b = setNames(aggregate(spatialdata$sd_expFirst10, by=list(spatialdata$condition, spatialdata$subject), mean), c("condition", "subject", "avg"))
# sd_c = setNames(aggregate(spatialdata$sd_expLast10, by=list(spatialdata$condition, spatialdata$subject), mean), c("condition", "subject", "avg"))
# 
# avg_sddata = rbind(cbind(sd_a, phase=rep("untimed practice", nrow(sd_a))), 
#                     cbind(sd_b, phase=rep("deadline early", nrow(sd_b))), 
#                     cbind(sd_c, phase=rep("deadline late", nrow(sd_c))))
# avg_sddata$phase = as.factor(avg_sddata$phase)         # Convert phase to factor
# levels(avg_sddata$condition) = c("fixed", "random") # Replace condition names

```

<!-- The following plot shows the distribution of participants' spatial strategy use. Higher proportions indicate responses which are more consistent with a spatial strategy. These figures indicate that most participants do not use a spatial strategy. when difficulty is maintained in a fixed location, two groups emerge under a deadline: those who do not use a spatial strategy, though there is a slightly higher frequency of spatial strategy use in the random condition. -->

```{r plotSpatialData, echo=FALSE, warning=FALSE, message=FALSE, results="asis"}
# spatial_choice_plot <- avg_scdata %>% ggplot(aes(x=avg, fill=phase)) + geom_histogram(color="white", alpha=0.6, bins = 10, position="identity") + facet_wrap(~condition+phase) + labs(y="Frequency", x = "Proportion of Spatially Consistent Responses") + ggtitle("Spatial Strategy Analysis")
# 
# #avg_plot <- avg_avgdata %>% ggplot(aes(x=avg, fill=phase+phase)) + geom_histogram(color="white", alpha=1, bins = 10) + facet_wrap(~condition+phase)
# 
# print(spatial_choice_plot)

```


```{r otherAnalyses, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
# Use package mixedMem or PLMIX to estimate mixtures of the PlackettLuce model with different groups having different coefficients. 
# Use PLMIX to estimate the PlackettLuce model using Bayes
```

```{r detectMissingSelectionData, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
# # Cycle through every trial, from every subject, from each phase, of every condition 
# # Determine if the recorded RDK data matches the selection data
# conditions = c("fixed", "random")
# phases = c("untimed", "deadline")
# max_trials = c(9, 29) # number of trials in the untimed and deadline conditions
# # finalSubjects - list of final subject numbers (uniqueIDs)
# 
# 
# udata <- data %>% filter(phase == "untimed")
# ddata <- data %>% filter(phase == "deadline")
# 
# 
# troubleMakers = setNames(data.frame(matrix(ncol = 4, nrow = 0)), c("condition", "uniqueid", "phase", "trial_number"))
# rcnt= 1;
# 
# cnt = 1
# total = sum(final_n$n) * 10
# pb <- txtProgressBar(min = 0, max = total, style = 3)
# # Cycle through untimed data first
# for (i in 1:sum(final_n$n)){
#   # Extract subject data
#   sdata <- udata %>% filter(uniqueid == finalSubjects$uniqueid[i])
#   
#   for (j in 0:max_trials[1]){
#     # Extract trial data
#     tdata <- sdata %>% filter(trial_number == j)
#     
#     nSelectionEvents = tdata %>% filter(trial_event == "practice_rdk") %>% count()
#     nRdkEvents = tdata %>% filter(trial_event == "rdk") %>% select(dot_coherence) %>% unique() %>% count()
#     
#     if (nRdkEvents <= nSelectionEvents){ # do nothing
#     } else {
#       # Add current subject, condition, phase, and trial to the list of troublesome trials      
#       troubleMakers[rcnt, ] = c(tdata$condition[1], tdata$uniqueid[1], tdata$phase[1], tdata$trial_number[1])
#       rcnt = rcnt + 1
#     }
#   
#   cnt = cnt + 1
#   setTxtProgressBar(pb, cnt)
#   }
# }
# 
# 
# cnt = 1
# total = sum(final_n$n) * 30
# pb <- txtProgressBar(min = 0, max = total, style = 3)
# # Cycle through untimed data first
# for (i in 1:sum(final_n$n)){
#   # Extract subject data
#   sdata <- ddata %>% filter(uniqueid == finalSubjects$uniqueid[i])
#   
#   for (j in 0:max_trials[2]){
#     # Extract trial data
#     tdata <- sdata %>% filter(trial_number == j)
#     
#     nSelectionEvents = tdata %>% filter(trial_event == "select_rdk") %>% count()
#     nRdkEvents = tdata %>% filter(trial_event == "rdk") %>% select(dot_coherence) %>% unique() %>% count()
#     
#     if (nRdkEvents <= nSelectionEvents){ # do nothing
#     } else {
#       # Add current subject, condition, phase, and trial to the list of troublesome trials      
#       troubleMakers[rcnt, ] = c(tdata$condition[1], tdata$uniqueid[1], tdata$phase[1], tdata$trial_number[1])
#       rcnt = rcnt + 1
#     }
#   
#   cnt = cnt + 1
#   setTxtProgressBar(pb, cnt)
#   }
# }

```

```{r notes, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
#files_to_build <- list.files("analysis", pattern = "^analysis_exp1_.*\\.Rmd$", full.names = TRUE)
#wflow_build(files = files_to_build)
#wflow_publish("analysis/analysis_exp1_labelled_nodelay.Rmd", message = "Update analysis code to use shared file repo")
#wflow_git_push()
```
