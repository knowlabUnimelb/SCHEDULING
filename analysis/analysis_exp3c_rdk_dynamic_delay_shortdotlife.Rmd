---
title: "Experiment 3c: Dynamic RDK Direction Judgement, Short Dot Life, 4 Tasks, 500 msec Error Penalty"
author: "knowlabUnimelb"
date: "2021-06-16"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

Ruby Steinberg^1^, Ami Eidels^2^, and Daniel R. Little^1^


^1^ The University of Melbourne, ^2^ The University of Newcastle

```{r load_modules, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
library(tidyverse)
library(workflowr)
#library(Rankcluster)
#library(rankdist) # Crashes rStudio
#library(Kendall)
library(DescTools) # ConDisPairs looks useful
library(gtools) 
library(english)
library(ggplot2)
library(dplyr)
library(hrbrthemes)
library(knitr)
library(reshape2)
library(png)
library(grid)
library(lme4)
library(rstatix)
library(jpeg)
library(pmr)
```

```{r load_data, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
rm(list=ls())
#dev.off(dev.list()["RStudioGD"])

#define working directories
inputdir <- "data"

# NEW 2/11/22 Reading data and variable names 
colfile       <- read.csv(paste(inputdir,"data_dictionary_shortdotlife.csv",sep="/"), stringsAsFactors = FALSE)
datafilename = "exp3c_rdk_data_dynamic_shortdotlife.csv"
datafn = paste(inputdir, datafilename,sep="/") 
rawdata = read.csv(datafn, header = FALSE, col.names = colfile$Column, colClasses = colfile$Type, stringsAsFactors = FALSE, na.strings='NA') # Add column labels to data


# OLD
# fixed_datafilename = "210616_fixed_moving_shortdotlife_data_corrected.csv"
# random_datafilename = "210616_random_moving_shortdotlife_data_corrected.csv"
# fixed_datafn  <- paste(inputdir,fixed_datafilename,sep="/") 
# random_datafn <- paste(inputdir,random_datafilename,sep="/") 
# fixd_rawdata  <- read.csv(fixed_datafn, header = FALSE, col.names = colfile$Column, colClasses = colfile$Type, stringsAsFactors = FALSE, na.strings='NULL') # Add column labels to data
# rand_rawdata    <- read.csv(random_datafn, header = FALSE, col.names = colfile$Column, colClasses = colfile$Type, stringsAsFactors = FALSE, na.strings='NULL') # Add column labels to data
# rawdata = rbind(fixd_rawdata, rand_rawdata)

# Handful of subjects who partially completed the experiment without an id, remove them
rawdata = rawdata[!is.na(rawdata$uniqueid), ]

# Fix some specific information here
levels(rawdata$condition) = c("fixed", "random") # Relabel to make easier to type
rawdata$correct[is.na(rawdata$correct)] = 0 # Incorrect trials were recorded as NA

# Summary data
loggedSubjects = rawdata %>% distinct(condition, uniqueid)
nLoggedSubjects = rawdata %>% distinct(condition, uniqueid) %>% count(condition)

```

```{r data_cleaning, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}

# Separate demographic information from data
uncleandata = rawdata[rawdata$trial_event != "demographics", ]

# A full data set has 10 practice trials (0 to 9) and 30 deadline trials (0 to 29)
# A full data set will have a max trial number of 29 
nTotalTrials = 29 # Counting from trial 0

completeSubjects = uncleandata[uncleandata$trial_number == nTotalTrials, ] %>% distinct(condition, subject)
nCompleteSubjects = uncleandata[uncleandata$trial_number == nTotalTrials, ] %>% distinct(condition, subject) %>% count(condition)

# Keep only complete datasets
cleandata = uncleandata[uncleandata$subject %in% completeSubjects$subject, ]

# some subjects may have completed the experiment twice, keep only first completion
completionIds = cleandata %>% distinct(uniqueid, subject)
completionCounts = cleandata %>% distinct(uniqueid, subject) %>% count(uniqueid)
repeatedIds = completionCounts$uniqueid[completionCounts$n > 1]

toDeleteSecondEntry = completionIds[completionIds$uniqueid %in% repeatedIds, ] %>% filter(duplicated(uniqueid))
keeplist = anti_join(completionIds, toDeleteSecondEntry)

# keep only nonrepeated subs
cleandata = cleandata[cleandata$subject %in% keeplist$subject, ]
subjects = cleandata %>% distinct(condition, uniqueid)
nSubjects = cleandata %>% distinct(condition, uniqueid) %>% count(condition)

```

```{r add_phase_col, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}

# Add in phase column which indicates whether current trials are practice or experiment
add_phase_col = function(data, subs){
  nSubs = length(subs)
  data$phase = rep(NA, nrow(data))
  for (i in 1:nSubs){
    
    # Find point at which trial_index = 0 later in the trial
    tidx = which(data$trial_number[data$uniqueid == subs[i]] == 0)
    idx = tidx[which(diff(tidx) > 1) + 1]
    
    dsize = nrow(data[data$uniqueid == subs[i], ])
    data$phase[data$uniqueid == subs[i]] = c(rep(0, idx-1), rep(1,length(idx:dsize)))
  }
  return(data)
}
cleandata = add_phase_col(cleandata, subjects$uniqueid)
cleandata$phase = as.factor(cleandata$phase)
levels(cleandata$phase) = c("untimed", "deadline")


```

```{r emails, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
# Need to extract emails for sending results from all datasets
# Turned off due to ethics

demographics = rawdata[rawdata$trial_event == "demographics", ]
#emails = demographics[grep("Email", demographics)]
#colon_loc = gregexpr(pattern = ":", emails)
#full_email_list = rep(NA, nSubjects)
#for (i in 1:nSubjects){
#  if (!is.na(emails[i])){
#    full_email_list[i] = substr(emails[i], colon_loc[[i]][1]+2, nchar(emails[i])-2)
#  }
#}
#email_list = str_sort(full_email_list[full_email_list != "" & !is.na(full_email_list)])

```

```{r demographics, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
# Demographics
# trial_event = sex_age
# responses --> Gender and Age

sexage = demographics$responses[grep("Gender", demographics$responses)]
  
colon_loc = gregexpr(pattern = ":", sexage)
comma_loc = gregexpr(pattern = ",", sexage)

nSubs = sum(nSubjects$n)  
sex = rep(NA, nSubs)
age = rep(NA, nSubs)
for (i in 1:nSubs){
  sex[i] = toupper(substr(sexage[i], colon_loc[[i]][1]+2, comma_loc[[i]][1]-2))
  
  x = substr(sexage[i], colon_loc[[i]][2]+2, nchar(sexage[i])-2)
  if (nchar(x) == 2){
    age[i] = as.numeric(x)
  } else {
    age[i] = as.numeric(substr(sexage[i], colon_loc[[i]][2]+2, colon_loc[[i]][2]+4))    
  }
}

demo_data = subjects %>% mutate(age, sex)  

nFemales = demo_data %>% filter(sex == "FEMALE" | sex == "WOMEN") %>% distinct(condition, uniqueid) %>% count(condition)
nMales   = demo_data %>% filter(sex == "MALE") %>% distinct(condition, uniqueid) %>% count(condition)
nUnspec  = demo_data %>% filter(sex != "FEMALE" & sex != "WOMEN" & sex != "MALE") %>% distinct(condition, uniqueid) %>% count(condition)
 
meanAge = demo_data %>% group_by(condition) %>% summarise(mean = mean(age, na.rm = TRUE))
stdAge = demo_data %>% group_by(condition) %>% summarise(sd = sd(age, na.rm = TRUE))

```
# Method

## Participants

[NEED TO ADD A SUMMARY OF PARTICIPOANT INFORMATION: HOW MANY TESTED, DEMOGRAPHIC INFORMATION, HOW WERE PARTICIPANTS REIMBURSED, HOW MANY ASSIGNED TO EACH CONDITION]


## Design

```{r getCoherenceSet, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
# Get all coherence options
# .8 = Easy, .5 = Medium, .2 = Hard, .0 = Very hard
coherence_set = sort(unique(c(cleandata$patch_0, cleandata$patch_1, cleandata$patch_2, cleandata$patch_3)), decreasing = TRUE)

```

[NEED TO ADD A DESCRIPTION OF THE EXPERIMENTAL DESIGN]

_Data Cleaning_

Subjects completed the experiment by clicking a link with the uniquely generated id code. Subjects were able to use the link multiple times; further, subjects were able to exit the experiment at any time. Consequently, the datafile contains partially completed data for some subjects which needed to be identified and removed. 


```{r analyse_RDK_difficulty, echo=FALSE, warning=FALSE, message=FALSE, results="asis"}
rdk = cleandata %>% filter(trial_event == 'rdk')

# Total rdk timeouts
nTimeouts = rdk %>% filter(rt == -1) %>% count()
pTimeouts = nTimeouts$n/nrow(rdk)

# Timeouts by subject
timeoutsPerSubject = rdk %>% filter(rt == -1) %>% count(uniqueid)
avgTimeoutPerSubject = mean(timeoutsPerSubject$n)

# Remove rdk timeouts
rdk = rdk %>% filter(rt != -1) 

# Analyse long rts
longRTcutoff = 3000
nlongrts = rdk %>% filter(rt > longRTcutoff) %>% count(phase)

# Remove long rts
# rdk = rdk %>% filter(rt <= longRTcutoff)

```

```{r identify_nonlearners, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}

easyacc = rdk %>% filter(dot_coherence == .8) %>% group_by(condition, phase, uniqueid) %>% summarise(mean = mean(correct))
nonlearners = easyacc %>% filter(mean < .4)
nNonlearners = nonlearners$uniqueid %>% unique() %>% length()

# Remove nonlearners
data = cleandata %>% filter(!uniqueid %in% nonlearners$uniqueid)
rdk = rdk %>% filter(!uniqueid %in% nonlearners$uniqueid)

# Count remaining subjects
finalSubjects = data %>% distinct(condition, uniqueid)
final_n = data %>% distinct(condition, uniqueid) %>% count(condition)

```

[NEED TO ADD A DESCRIPTION OF THE NONLEARNERS]

```{r task_completion, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
# tcdata = aggregate(rdk$trial_event[rdk$correct == 1], by=list("condition" = rdk$condition[rdk$correct == 1], "trial" = rdk$trial_number[rdk$correct == 1], "phase" = rdk$phase[rdk$correct == 1], "subject" = rdk$uniqueid[rdk$correct == 1]), FUN=length)

tcdata = rdk %>% filter(correct == 1) %>% group_by(condition, trial_number, phase, uniqueid) %>% count()
avgCompletions = rdk %>% filter(correct == 1) %>% group_by(condition, trial_number, phase, uniqueid) %>% count() %>% group_by(condition, phase) %>% summarise(mean = mean(n, na.rm=TRUE))

#anova.comp = tcdata %>% ungroup %>% anova_test(dv = n, wid = uniqueid, within = phase, between = condition)

```

# Data Analysis

We first summarize performance by answering the following questions: 

## Task completions

* How many tasks are completed on average?

[ADD ANALYSIS DESCRIPTION]

```{r task_completion_table, echo=FALSE, warning=FALSE, message=FALSE, results="asis"}
# Reorder columns
avgCompletions = avgCompletions[c("condition", "phase", "mean")]
kable(avgCompletions, caption="Average number of correctly completed tasks in each condition")
```

```{r task_completion_analysis, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
# TODO: Rewrite using tidyverse
tc.aov = aov(n ~ condition + phase + condition:phase, data = tcdata)
tc.ml0 = lm(n ~ condition + phase + condition:phase, data = tcdata)
tc.ml1 = lmer(n ~ condition + phase + condition:phase + (1|uniqueid), data = tcdata)
tc.ml2 = lmer(n ~ condition + phase + condition:phase + (1+phase|uniqueid), data = tcdata)

compare.1 = anova(tc.ml1, tc.ml0, test="chisq")
compare.2 = anova(tc.ml2, tc.ml1, test="chisq")

# Comparison indicates that tc.ml2 is the preferred model on a BIC basis
tc.ml2.summary = summary(tc.ml2)

```

[ADD DESCRIPTION OF ANALYSIS OF TASK COMPLETIONS]

## RDK performance

```{r rdk_anova, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
# TODO: Rewrite using tidyverse
aov.acc.data = aggregate(list("accuracy" = rdk$correct), by = list("subject" = rdk$uniqueid, "condition" = rdk$condition, "phase" = rdk$phase, "difficulty" = rdk$dot_coherence), FUN = function(x) mean(x, na.rm=TRUE))

aov.rt.data = aggregate(list("rt" = rdk$rt), by = list("subject" = rdk$subject, "condition" = rdk$condition, "phase" = rdk$phase, "difficulty" = rdk$dot_coherence), FUN = function(x) mean(x, na.rm=TRUE))

# Note that RT is computed by averaging every attempt
# Another way to compute RT is by summing over incorrect attempts until the RDK is responded to correctly

rdk.crt = aggregate(list("crt" = rdk$rt), by = list("subject"=rdk$subject, "condition"=rdk$condition, "phase"=rdk$phase, "difficulty"=rdk$dot_coherence, "trial"=rdk$trial_number), FUN = function(x) tail(cumsum(x), n=1))

# rdk.crt = rdk.crt[rdk.crt$crt <= 6000, ]

aov.crt.data = aggregate(list("rt" = rdk.crt$crt), by = list("subject" = rdk.crt$subject, "condition" = rdk.crt$condition, "phase" = rdk.crt$phase, "difficulty" = rdk.crt$difficulty), FUN = function(x) mean(x, na.rm=TRUE))

#anova.acc = glm(accuracy ~ condition + phase + difficulty + condition:phase:difficulty, aov.acc.data, family = "gaussian")
#anova.rt = glm(rt ~ condition + phase + difficulty + condition:phase:difficulty, aov.rt.data, family = "gaussian")

anova.acc = anova_test(data = aov.acc.data, dv = accuracy, wid = subject, within = c(phase, difficulty), between = condition)
anova.rt = anova_test(data = aov.rt.data, dv = rt, wid = subject, within = c(phase, difficulty), between = condition)
anova.crt = anova_test(data = aov.crt.data, dv = rt, wid = subject, within = c(phase, difficulty), between = condition)

# Linear contrasts
aov.acc.data.linear <- aov.acc.data %>%
  mutate(diffnum = difficulty %>% recode(`0.0` = 3, `0.2` = 2, `0.5` = 1, `0.8` = 0),
         linear = diffnum %>% recode(`0` = -3, `1` = -1, `2` = 1, `3` = 3))
contrasts.acc <- lm(accuracy ~ linear + subject, data = aov.acc.data.linear)
summary(contrasts.acc)

aov.acc.data.linear <- aov.acc.data %>%
  mutate(diffnum = difficulty %>% recode(`0.0` = 3, `0.2` = 2, `0.5` = 1, `0.8` = 0),
         linear = diffnum %>% recode(`0` = -3, `1` = -1, `2` = 1, `3` = 3))
contrasts.acc <- lm(accuracy ~ linear + subject, data = aov.acc.data.linear)
summary(contrasts.acc)

aov.rt.data.linear <- aov.rt.data %>%
  mutate(diffnum = difficulty %>% recode(`0.0` = 3, `0.2` = 2, `0.5` = 1, `0.8` = 0),
         linear = diffnum %>% recode(`0` = -3, `1` = -1, `2` = 1, `3` = 3))
contrasts.rt <- lm(rt ~ linear + subject, data = aov.rt.data.linear)
summary(contrasts.rt)

aov.crt.data.linear <- aov.crt.data %>%
  mutate(diffnum = difficulty %>% recode(`0.0` = 3, `0.2` = 2, `0.5` = 1, `0.8` = 0),
         linear = diffnum %>% recode(`0` = -3, `1` = -1, `2` = 1, `3` = 3))
contrasts.crt <- lm(rt ~ linear + subject, data = aov.crt.data.linear)
summary(contrasts.crt)


```

We next analysed performance on the RDK discriminations. We then asked:

* What was the average completion time and accuracy of the easy, medium, hard, and very hard tasks? 

[ADD DESCRIPTION OF PLOT]

```{r difficulty_plot, echo=FALSE, warning=FALSE, message=FALSE, results="asis"}

# Difficulty plot
rdk$labels = factor(rdk$dot_coherence)
levels(rdk$labels) = c("v. hard", "hard", "medium", "easy")
rdk$labels = reorder(rdk$labels, new.order = c("easy", "medium", "hard", "v. hard"))

# Trim long tails on violin plot
pct = quantile(rdk$rt, c(.025, .975))
trim_rdk = rdk %>% filter(rt > pct[1] & rt < pct[2])

diff_plot <- trim_rdk %>% ggplot(aes(x=labels, y=rt, fill=labels)) + geom_violin(adjust = 1.5, trim = TRUE, scale="area") + geom_jitter(height = 0, width = 0.05) + facet_wrap(~phase) + labs(y="RT (msec)", x = "Difficulty")
print(diff_plot + ggtitle("Response times as a function of difficulty"))
```

[ADD DESCRIPTION OF PLOT]

```{r difficulty_sum_plot, echo=FALSE, warning=FALSE, message=FALSE, results="asis"}
# RT summed over additional attempts
rdk.crt$labels = as.factor(rdk.crt$difficulty)
levels(rdk.crt$labels) = c("v. hard", "hard", "medium", "easy")
rdk.crt$labels = reorder(rdk.crt$labels, new.order = c("easy", "medium", "hard", "v. hard"))

# Trim long tails on violin plot
spct = quantile(rdk.crt$crt, c(.025, .975))
trim_crdk = rdk.crt %>% filter(crt > spct[1] & crt < spct[2])

crt_diff_plot <- trim_crdk %>% ggplot(aes(x=labels, y=crt, fill=labels)) + geom_violin(adjust = 1.5, trim = TRUE, scale="area") + geom_jitter(height = 0, width = 0.05) + facet_wrap(~phase) + labs(y="RT (msec)", x = "Difficulty")
print(crt_diff_plot + ggtitle("Response times summed over attempts as a function of difficulty"))

```

We further broke down RTs by condition, deadline, and difficulty. 

```{r difficulty_data, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
# TODO: recode to tidyverse
# Find average rdk accuracy and rt in each codnition, phase, and coherence level
difficulty = aggregate(list("accuracy" = rdk$correct, "rt" = rdk$rt), by = list("difficulty" =  rdk$labels, "phase" = rdk$phase, "condition" = rdk$condition), FUN = function(x) mean(x, na.rm = TRUE))
sum_difficulty = aggregate(list("crt" = rdk.crt$crt), by = list("difficulty" = rdk.crt$labels, "phase" = rdk.crt$phase, "condition" = rdk.crt$condition), FUN = function(x) mean(x, na.rm = TRUE))
                
diffsd = aggregate(list("accuracy" = rdk$correct, "rt" = rdk$rt), by = list("difficulty" = rdk$labels, "phase" = rdk$phase, "condition" = rdk$condition), FUN = function(x) sd(x, na.rm = TRUE))
sum_diffsd = aggregate(list("crt" = rdk.crt$crt), by = list("difficulty" =  rdk.crt$labels, "phase" = rdk.crt$phase, "condition" = rdk.crt$condition), FUN = function(x) sd(x, na.rm = TRUE))

ncounts = rdk %>% count(condition, phase, dot_coherence)
difficulty$n = ncounts$n
difficulty$accuracy = round(difficulty$accuracy, 2)
difficulty$rt = round(difficulty$rt, 2)
difficulty$se.acc = diffsd$accuracy/sqrt(difficulty$n)
difficulty$se.acc = round(difficulty$se.acc, 2)
difficulty$se.rt = diffsd$rt/sqrt(difficulty$n)
difficulty$se.rt = round(difficulty$se.rt, 2)
difficulty$crt = round(sum_difficulty$crt, 2)
difficulty$se.crt = sum_diffsd$crt/sqrt(difficulty$n)
difficulty$se.crt = round(difficulty$se.crt, 2)

# Get reward rate standard error
rrdata = aggregate(list("accuracy" = rdk$correct), by = list("difficulty" = rdk$labels, "phase" = rdk$phase, "condition" = rdk$condition, "subject"=rdk$subject), FUN = function(x) mean(x, na.rm = TRUE))
rrdata_crt = aggregate(list("crt" = rdk.crt$crt), by = list("difficulty" = rdk.crt$labels, "phase" = rdk.crt$phase, "condition" = rdk.crt$condition, "subject"=rdk.crt$subject), FUN = function(x) mean(x, na.rm = TRUE))


rrdata$crt = rrdata_crt$crt
rrdata$rewardRate = rrdata$accuracy/(rrdata$crt/1000)

subRewardRate = aggregate(list("rr" = rrdata$rewardRate), by = list("difficulty" =  rrdata$difficulty, "phase" = rrdata$phase, "condition" = rrdata$condition), FUN = function(x) mean(x, na.rm = TRUE))

subRewardRate.se = aggregate(list("rr" = rrdata$rewardRate), by = list("difficulty" =  rrdata$difficulty, "phase" = rrdata$phase, "condition" = rrdata$condition), FUN = function(x) sd(x, na.rm = TRUE))

difficulty$rewardRate = subRewardRate$rr
difficulty$SE.rewardRate = subRewardRate.se$rr/sqrt(difficulty$n)


difficulty$difficulty = as.factor(difficulty$difficulty)
#levels(difficulty$difficulty) = c("Very Hard", "Hard", "Medium", "Easy")
```


```{r difficulty_table, echo=FALSE, warning=FALSE, message=FALSE, results="asis"}
# TODO: recode to tidyverse

# Create the table
difficulty = difficulty[c("condition", "phase", "difficulty", "n", "accuracy", "se.acc", "rt", "se.rt", "crt", "se.crt")]
colnames(difficulty)[colnames(difficulty) == "accuracy"] = "Mean.Correct"
colnames(difficulty)[colnames(difficulty) == "se.acc"] = "SE.Correct"
colnames(difficulty)[colnames(difficulty) == "rt"] = "Mean.RT"
colnames(difficulty)[colnames(difficulty) == "se.rt"] = "SE.RT"
colnames(difficulty)[colnames(difficulty) == "crt"] = "Mean.crt"
colnames(difficulty)[colnames(difficulty) == "se.crt"] = "SE.crt"
knitr::kable(difficulty, caption="Mean accuracy, RT, and RT summed across attempts for each difficulty and each phase")

# Marginal means
# aggregate(rdk$correct, by=list(rdk$condition), FUN=mean)
# aggregate(rdk$correct, by=list(rdk$phase), FUN=mean)
# aggregate(rdk$correct, by=list(rdk$difficulty), FUN=mean)
```



## Reward Rate

[ADD  DESCRIPTION]

```{r rewardRate_plot, echo=FALSE, warning=FALSE, message=FALSE, results="asis"}
rr_plot <- rrdata %>% ggplot(aes(x=difficulty, y=rewardRate, fill=difficulty)) + geom_violin(adjust = 1.5, trim = TRUE) +  stat_summary(fun=mean, geom="point", shape=23, size=2) + geom_jitter(height = 0, width = 0.05) + facet_wrap(~phase+condition) + labs(y="Reward Rate = p(Correct)/RT(sec)", x = "Difficulty") + lims(y = c(0, 2.5))

print(rr_plot + ggtitle("Reward Rate as a function of difficulty"))

```


## Optimality in each condition


* What is the proportion of easy, medium, hard, and very hard tasks selected first, second, third or fourth? 
[ADD DESCRIPTION]

```{r get_selections, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
numberOfTasks = 4 # Number of tasks

selection_data = data %>% filter(trial_event %in% c("practice_rdk", "select_rdk"))
selection_data$trial_event = droplevels(selection_data$trial_event) # drop demographics level

ph = c(rep("untimed", 10), rep("deadline", 30))
cn = c(rep("practice_rdk", 10), rep("select_rdk", 30))
tr = c(seq(0,9), seq(0,29))
sdLength = length(cn) # selection data length for each subject

# Convert location selections to difficulty order selections
selections = data.frame(subject = rep(finalSubjects$uniqueid, each = sdLength), condition = rep(finalSubjects$condition, each = sdLength), phase = rep(ph, sum(final_n$n)), trial = rep(tr, sum(final_n$n)), selected_difficulty_1 = rep(NA, sum(final_n$n) * sdLength), selected_difficulty_2 = rep(NA, sum(final_n$n) * sdLength), selected_difficulty_3 = rep(NA, sum(final_n$n) * sdLength), selected_difficulty_4 = rep(NA, sum(final_n$n) * sdLength), rt_1 = rep(NA, sum(final_n$n) * sdLength), rt_2 = rep(NA, sum(final_n$n) * sdLength), rt_3 = rep(NA, sum(final_n$n) * sdLength), rt_4 = rep(NA, sum(final_n$n) * sdLength))
selections$phase <- factor(selections$phase, levels = c("untimed", "deadline"))


cnt = 1
total = sum(final_n$n) * sdLength
pb <- txtProgressBar(min = 0, max = total, style = 3)
for (i in 1:sum(final_n$n)){
    for (j in 1:sdLength){
      # Coherence in each location (W, N, E, S) 
      coherence_locations = selection_data %>% filter(uniqueid == finalSubjects$uniqueid[i], trial_event == cn[j], trial_number == tr[j]) %>% slice(1) %>% select(patch_0, patch_1, patch_2, patch_3)
      difficulty_selections = selection_data %>% filter(uniqueid == finalSubjects$uniqueid[i], trial_event == cn[j], trial_number == tr[j]) %>% pull(selected_difficulty) + 1

      # coherence_selections = as.numeric(as.vector(coherence_locations)[location_selections[!is.na(location_selections)]])
      # length(coherence_selections) <- numberOfTasks # pad end with NAs
      
      selection_rt = selection_data %>% filter(uniqueid == finalSubjects$uniqueid[i], trial_event == cn[j], trial_number == tr[j]) %>% pull(rt)
      length(selection_rt) <- numberOfTasks # pad end with NAs
      
      selections$selected_difficulty_1[cnt] = difficulty_selections[1]
      selections$selected_difficulty_2[cnt] = difficulty_selections[2]
      selections$selected_difficulty_3[cnt] = difficulty_selections[3]
      selections$selected_difficulty_4[cnt] = difficulty_selections[4]
      
      selections$rt_1[cnt] = selection_rt[1]
      selections$rt_2[cnt] = selection_rt[2]
      selections$rt_3[cnt] = selection_rt[3]
      selections$rt_4[cnt] = selection_rt[4]

      cnt = cnt + 1
      setTxtProgressBar(pb, cnt)
    }
}
close(pb)

selections$rt_1[selections$rt_1 < 0] = NA
selections$rt_2[selections$rt_2 < 0] = NA
selections$rt_3[selections$rt_3 < 0] = NA
selections$rt_4[selections$rt_4 < 0] = NA

```

```{r find_heatmap, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
fixed_untimed   = selections %>% filter(condition == "fixed", phase == "untimed") %>% select(selected_difficulty_1, selected_difficulty_2, selected_difficulty_3, selected_difficulty_4) %>% as.matrix()
fixed_deadline  = selections %>% filter(condition == "fixed", phase == "deadline") %>% select(selected_difficulty_1, selected_difficulty_2, selected_difficulty_3, selected_difficulty_4) %>% as.matrix()
random_untimed  = selections %>% filter(condition == "random", phase == "untimed") %>% select(selected_difficulty_1, selected_difficulty_2, selected_difficulty_3, selected_difficulty_4) %>% as.matrix()
random_deadline = selections %>% filter(condition == "random", phase == "deadline") %>% select(selected_difficulty_1, selected_difficulty_2, selected_difficulty_3, selected_difficulty_4) %>% as.matrix()

imputeMissingData = function(data, n = numberOfTasks){
  for (i in 1:n){
      data[is.na(data[,i]), i] = mean(data[,i], na.rm=TRUE)
  }
  return(data)
}

fixed_untimed = imputeMissingData(fixed_untimed, numberOfTasks)
fixed_deadline = imputeMissingData(fixed_deadline, numberOfTasks)
random_untimed = imputeMissingData(random_untimed, numberOfTasks)
random_deadline = imputeMissingData(random_deadline, numberOfTasks)

fu = fixed_untimed %>% rankagg() %>% destat()
fd = fixed_deadline %>% rankagg() %>% destat()
ru = random_untimed %>% rankagg() %>% destat()
rd = random_deadline %>% rankagg() %>% destat()

```

```{r heatmap_plot, echo=FALSE, warning=FALSE, message=FALSE, results="asis"}

# Convert to proportions
setUpMat = function(matData){
  mat = t(apply(matData, 1, function(x) x/sum(x)))  
  matMelt = melt(mat)
  names(matMelt) = c("Difficulty", "Order", "value")
  return(matMelt)
}
fumelt = setUpMat(fu$mar)
fdmelt = setUpMat(fd$mar)
rumelt = setUpMat(ru$mar)
rdmelt = setUpMat(rd$mar)

hm = rbind(cbind("condition" = rep("Fixed Untimed", nrow(fumelt)), fumelt), 
           cbind("condition" = rep("Fixed Deadline", nrow(fdmelt)), fdmelt),
           cbind("condition" = rep("Random Untimed", nrow(rumelt)), rumelt),
           cbind("condition" = rep("Random Deadline", nrow(rdmelt)), rdmelt))

hm$condition = factor(hm$condition, levels= c("Fixed Untimed", "Fixed Deadline", "Random Untimed", "Random Deadline"))


hmplot = ggplot(hm, aes(x=Difficulty, y=Order)) +
          geom_tile(aes(fill=value)) +
          scale_fill_gradientn(colours=c("blue","pink", "red")) +
          geom_text(aes(label=round(value, 2))) + 
          scale_y_reverse() +
          facet_wrap(~condition) + 
          labs(y="Selection", x = "Difficulty")
print(hmplot)

```

* Do the marginal distributions differ from uniformity?

We tested whether the marginal distributions were different from uniformally random selection using the fact that the mean rank is distributed according to a  $\chi^2$ distribution with the following test-statistic:
$$\chi^2 = \frac{12N}{k(k+1)}\sum_{j=1}^k \left(m_j - \frac{k+1}{2} \right)^2$$
see (Marden, 1995). 

```{r chi2uniformity, echo=FALSE, warning=FALSE, message=FALSE, results="asis"}
selection_n = selections %>% count(condition, phase) 

# Compare heatmaps to uniform
observed <- fu$mean.rank

chi2UniformRank = function(observed, n, k = numberOfTasks){
  ex <- rep((k + 1)/2, k);
  oc <- observed
  
  chi = (12 * n/(k * (k + 1))) * sum((oc - ex)^2) # Test statistic of mean rank under uniformity
  p = dchisq(chi, k-1)
  return(list("chi2" = chi, "chi2p" = p))
}

fu$uni <- chi2UniformRank(fu$mean.rank, selection_n$n[selection_n$condition == "fixed" & selection_n$phase == "untimed"])
fd$uni <- chi2UniformRank(fd$mean.rank, selection_n$n[selection_n$condition == "fixed" & selection_n$phase == "deadline"])
ru$uni <- chi2UniformRank(ru$mean.rank, selection_n$n[selection_n$condition == "random" & selection_n$phase == "untimed"])
rd$uni <- chi2UniformRank(rd$mean.rank, selection_n$n[selection_n$condition == "random" & selection_n$phase == "deadline"])

chiUniformTable = data.frame("condition" = c("fixed", "fixed", "random", "random"), 
                            "phase" = c("untimed", "deadline", "untimed", "deadline"), 
                            "chi2" = round(c(fu$uni$chi2, fd$uni$chi2, ru$uni$chi2, rd$uni$chi2),2), 
                            "df" = c(3,3,3,3),
                            "p" = round(c(fu$uni$chi2p, fd$uni$chi2p, ru$uni$chi2p, rd$uni$chi2p), 2))

knitr::kable(chiUniformTable, caption = "Chi2 test of uniformity")

```


[ADD DESCRIPTION]

We compared the location conditions and phases using chi-2 analysis.

```{r chi2btw, echo=FALSE, warning=FALSE, message=FALSE, results="asis"}
# Compare heatmaps to each other
chi2_f_UvsD = chisq.test(cbind(as.vector(fu$mar),as.vector(fd$mar))) # Fixed: Untimed vs Deadline
chi2_r_UvsD = chisq.test(cbind(as.vector(ru$mar),as.vector(rd$mar))) # Random: Untimed vs Deadline
chi2_u_FvsR = chisq.test(cbind(as.vector(fu$mar),as.vector(ru$mar))) # Untimed: Fixed vs Random
chi2_d_FvsR = chisq.test(cbind(as.vector(fd$mar),as.vector(rd$mar))) # Deadline: Fixed vs Random

chi2compTable = data.frame("Comparison" = c("Fixed: Untimed vs Deadline", "Random: Untimed vs Deadline", "Untimed: Fixed vs Random", "Deadline: Fixed vs Random"), 
                           "chi2" = round(c(chi2_f_UvsD$statistic, chi2_r_UvsD$statistic, chi2_u_FvsR$statistic, chi2_d_FvsR$statistic), 2),
                           "df" = c(chi2_f_UvsD$parameter, chi2_r_UvsD$parameter, chi2_u_FvsR$parameter, chi2_d_FvsR$parameter),
                           "p"  = round(c(chi2_f_UvsD$p.value, chi2_r_UvsD$p.value, chi2_u_FvsR$p.value, chi2_d_FvsR$p.value), 2))

knitr::kable(chi2compTable, caption = "Pearson's chi-squared test")


# MANOVA-like analysis to compare multiple groups? (Marsden book)




```


```{r distcomp, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
# Distance parameters
maxdistance = (numberOfTasks * (numberOfTasks-1))/2
missing_penalty = maxdistance/numberOfTasks
allperms = permutations(n=4, r=4, v=1:4, repeats.allowed=FALSE)
nperms = nrow(allperms)

# Compute distance from optimal for all perms
pdist <- rep(NA, nperms)
for (i in 1:nperms){
  distObj = ConDisPairs(table(allperms[i, ], c(1,2,3, 4)))
  pdist[i] = distObj$D  
}

# Find average distance based on all partial matches
scorePartialMatches = function(sv, allperms, pdist){
    selVector = as.numeric(sv)
    if (all(is.na(selVector))){ # If no tasks are selected
      avgd = mean(pdist)
      mind = min(pdist)
      maxd = max(pdist)      
    } else {
      if (sum(!is.na(selVector)) > 1){ # If more than one task is selected
        dvec = pdist[apply(t(replicate(nperms, selVector[!is.na(selVector)])) == allperms[, !is.na(selVector)], 1, function(x)all(x))]
      } else { # If only one task is selected
        dvec = pdist[selVector[!is.na(selVector)] == allperms[, !is.na(selVector)]]
      }
      avgd = mean(dvec)
      mind = min(dvec)
      maxd = max(dvec)
    }
    return(list("avgD" = avgd, "minD" = mind, "maxD" = maxd))
}

# Add average distance column
selections = selections %>% mutate("avgD" = apply(selections %>% select(selected_difficulty_1, selected_difficulty_2, selected_difficulty_3, selected_difficulty_4), 1, function(x)(scorePartialMatches(x, allperms, pdist)$avgD)))

# Add min distance column
selections = selections %>% mutate("minD" = apply(selections %>% select(selected_difficulty_1, selected_difficulty_2, selected_difficulty_3, selected_difficulty_4), 1, function(x)(scorePartialMatches(x, allperms, pdist)$minD)))

# Add max distance column
selections = selections %>% mutate("maxD" = apply(selections %>% select(selected_difficulty_1, selected_difficulty_2, selected_difficulty_3, selected_difficulty_4), 1, function(x)(scorePartialMatches(x, allperms, pdist)$maxD)))

```

```{r dist_analysis, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
# Compare distributions of distances from optimal
dist_fuVSfd = ks.test(selections %>% filter(condition == "fixed" & phase == "untimed") %>% pull(avgD), selections %>% filter(condition == "fixed" & phase == "deadline") %>% pull(avgD))

dist_fuVSru = ks.test(selections %>% filter(condition == "fixed" & phase == "untimed") %>% pull(avgD), selections %>% filter(condition == "random" & phase == "untimed") %>% pull(avgD))

dist_ruVSrd = ks.test(selections %>% filter(condition == "random" & phase == "untimed") %>% pull(avgD), selections %>% filter(condition == "random" & phase == "deadline") %>% pull(avgD))

dist_fdVSrd = ks.test(selections %>% filter(condition == "fixed" & phase == "deadline") %>% pull(avgD), selections %>% filter(condition == "random" & phase == "deadline") %>% pull(avgD))

ksCompTable = data.frame("Comparison" = c("Fixed: Untimed vs Deadline", "Random: Untimed vs Deadline", "Untimed: Fixed vs Random", "Deadline: Fixed vs Random"), 
                           "ks" = round(c(dist_fuVSfd$statistic, dist_ruVSrd$statistic, dist_fuVSru$statistic, dist_fdVSrd$statistic), 2),
                           "p"  = round(c(dist_fuVSfd$p.value, dist_ruVSrd$p.value, dist_fuVSru$p.value, dist_fdVSrd$p.value), 2))

knitr::kable(ksCompTable, caption = "Kolmolgorov-Smirnoff test")

```



```{r match_analysis, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
# Omit this

 # # Compute matches by subject for first selection, first two selections, first three selections
 #  match1 = aggregate(cbind(fullmat[,1], fullmat[,2]==fullopt[,2]), by=list(fullmat[,1]), FUN = function(x)mean(x,na.rm=TRUE))
 #  match2 = aggregate(cbind(fullmat[, 1], apply(fullmat[,2:3] == fullopt[,2:3], 1, all)), by=list(fullmat[,1]), FUN = function(x)mean(x,na.rm=TRUE))
 #  match3 = aggregate(cbind(fullmat[, 1], apply(fullmat[,2:4] == fullopt[,2:4], 1, all)), by=list(fullmat[,1]), FUN = function(x)mean(x,na.rm=TRUE))
 #  # match4 = aggregate(cbind(fullmat[, 1], apply(fullmat[,2:5] == fullopt[,2:5], 1, all)), by=list(fullmat[,1]), FUN = mean) # same as match3
 #  matches = as.data.frame(cbind(match1$V1, match1$V2, match2$V2, match3$V2))
 #  names(matches) = c("subjects", "match1", "match2", "match3")
```


<!-- * We next ask whether the sequence of choices reflected the optimal order: What is the proportion of easy-task-first choices in each condition? Of easy-then-medium? Of easy-medium-then-hard? This provides an indication of how the order of responding deviates from optimal in each condition. The table presents the proportion of subjects responding with each order; the figure presents the frequency with which subjects respond with the easiest first RDK, easiest then medium, and so on. -->

```{r analyseMatches, fig.height = 10, fig.width = 9, echo=FALSE, warning=FALSE, message=FALSE, results="asis"}
# avgMatch = aggregate(list("easy first"=match_mat$match1, "easy-med"=match_mat$match2, "easy-med-hard"=match_mat$match3), by = list("phase"=match_mat$phase, "condition"=match_mat$condition), FUN = function(x)mean(x,na.rm=TRUE))
# 
# avgMatch$easy.first = round(avgMatch$easy.first,2)
# avgMatch$easy.med = round(avgMatch$easy.med,2)
# avgMatch$easy.med.hard = round(avgMatch$easy.med.hard,2)
# 
# # Create the table
# kable(avgMatch, caption="Average optimal choices")
# 
# long_match = melt(match_mat, id.var = c("condition", "phase"), measure.var = c("match1", "match2", "match3"), variable.name = "match")
# levels(long_match$match) = c("Order = Easy", "Order = Easy, Medium", "Order = Easy, Medium, Hard")
# 
# match_plot <- long_match %>% ggplot(aes(x=value, fill=match)) + geom_histogram(color="white", alpha=0.6, bins = 10, position="identity") + facet_wrap(~condition+phase+match, nrow=4, ncol = 3) + labs(y="Participant Frequency", x = "Proportion of First Choices")
# 
# print(match_plot + ggtitle("Optimally Ordered Responses")) 

```

* How optimal were responses? 



```{r plotData, echo=FALSE, warning=FALSE, message=FALSE, results="asis"}
# max_plot <- avg_maxdata %>% ggplot(aes(x=avg, fill=phase)) + geom_histogram(color="white", alpha=0.6, bins = 10, position="identity") + facet_wrap(~condition+phase) + labs(y="Frequency", x = "Max Distance")
# print(max_plot + ggtitle("Max Distance"))
# 
# min_plot <- avg_mindata %>% ggplot(aes(x=avg, fill=phase)) + geom_histogram(color="white", alpha=0.6, bins = 10, position="identity") + facet_wrap(~condition+phase) + labs(y="Frequency", x = "Min Distance")
# print(min_plot + ggtitle("Min Distance"))

avg_plot <- selections %>% ggplot(aes(x=avgD, fill=phase)) + geom_histogram(aes(y=..density..), color="white", alpha=0.6, bins = 6, position="identity") + facet_wrap(~condition+phase) + labs(y="Probability", x = "Avg Distance")
print(avg_plot + ggtitle("Avg Distance"))


```

# Selection model

We can treat each task selection as a probabilistic choice given by a Luce's choice rule (Luce, 1959), where each task is represented by some strength, $\nu$. The probability of selecting task $i_j$ from set $S = \left{i_1, i_2, ..., i_J \right}$, where J is the number of tasks, is:

$$p\left(i_j |S \right) = \frac{\nu_{i_j}}{\sum_{i \in S} \nu_{i}} $$.

Plackett (1975) generalised this model to explain the distribution over a sequence of choices (i.e., ranks). In this case, after each choice, the choice set is reduce by one (i.e., sampling without replacement). This probability of observing a specific selection order, $i_1 \succ ... \succ i_J$ is:

$$p\left(i_j |A \right) = \prod_{j=1}^J \frac{\nu_{i_j}}{\sum_{i \in A_j} \nu_{i}} $$, 

where $A_j$ is the current choice set.


```{r PLmodel, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}

# Estimate Plackett-Luce model parameters

fu.pl = fixed_untimed %>% rankagg() %>% pl() # use fd.pl@details to access results
fd.pl = fixed_deadline %>% rankagg() %>% pl()
ru.pl = random_untimed %>% rankagg() %>% pl()
rd.pl = random_deadline %>% rankagg() %>% pl()

slopeLP = function(coef){
    # Fit a line to the coefficients
    df = data.frame('diff' = c(1:4), 'x' = coef)
    lmod = lm(x ~ diff, df)
    slope = round(lmod$coefficients['diff'], 4)
    return("slope" = abs(slope))
}

PlackettLuceParms = data.frame("condition" = c("fixed", "fixed", "random", "random"), 
                              "phase" = c("untimed", "deadline", "untimed", "deadline"), 
                              "Easy" = round(c(fu.pl@coef[1], fd.pl@coef[1], ru.pl@coef[1], rd.pl@coef[1]),2),
                              "Med" = round(c(fu.pl@coef[2], fd.pl@coef[2], ru.pl@coef[2], rd.pl@coef[2]),2),
                              "Hard" = round(c(fu.pl@coef[3], fd.pl@coef[3], ru.pl@coef[3], rd.pl@coef[3]),2),
                              "V. Hard" = round(c(fu.pl@coef[4], fd.pl@coef[4], ru.pl@coef[4], rd.pl@coef[4]),2), 
                              "Slope" = c(slopeLP(fu.pl@coef), slopeLP(fd.pl@coef), slopeLP(ru.pl@coef), slopeLP(rd.pl@coef)))
knitr::kable(PlackettLuceParms, caption = "Plackett-Luce strength parameter estimates")


plSample = function(coefs){
  coefs[coefs<0] <- 0
  J = length(coefs)
  A = 1:J
  v = coefs/sum(coefs)
  s = rep(NA, J)
  for (i in 1:(J-1)){
    s[i] = sample(A, 1, prob=v)
    v = v[A!=s[i]]
    v = v/sum(v)
    A = A[A!=s[i]]
  }
  s[J] = A
  return(s)
}

plSamples = function(coefs, n){
    s = matrix(NA, n, 4)
    for (i in 1:n){
        s[i,] = as.matrix(plSample(coefs))
    }
    return(s)
}

# TODO: Sample selection orders using pl coefficients
nsim = 10000
sim_pl = data.frame("condition" = c(rep("fixed", nsim*2), rep("random", nsim*2)),
                    "phase" = c(rep("untimed", nsim), rep("deadline", nsim), rep("untimed", nsim), rep("deadline", nsim)),
                    "sim" = rbind(plSamples(fu.pl@coef, nsim), plSamples(fd.pl@coef, nsim), plSamples(ru.pl@coef, nsim), plSamples(rd.pl@coef, nsim)))
sim_pl$phase <- factor(sim_pl$phase, levels=c('untimed', 'deadline'))

# Add average distance column
sim_pl = sim_pl %>% mutate("avgD" = apply(sim_pl %>% select(sim.1, sim.2, sim.3, sim.4), 1, function(x)(scorePartialMatches(x, allperms, pdist)$avgD)))

#       Compare resulting distribution with data
sim_avg_plot <- sim_pl %>% ggplot(aes(x=avgD, fill=phase)) + geom_histogram(aes(y=..density..), color="white", alpha=0.6, bins = 6, position="identity") + facet_wrap(~condition+phase) + labs(y="Probability", x = "Avg Distance")
print(sim_avg_plot + ggtitle("Avg Distance - Plackett-Luce Model"))
```


## Sampling distribution anlaysis

In order to characterise performance, we examined three sampling distributions for comparison to our data. The first is the sampling distribution of edit distances from optimal assuming that orders are sampled uniformly at random. The second distribution assumes that the first choice was optimal but the remaining orders are sampled at random. Finally, the third distributions assumes that the first two choices are selected optimally but that the remaining are randomly selected. It is clear that the mode of the distribution moves from a distance of 3 to a distance of 0 as the sampling distribution summarises orders which better conform to optimality. 

To characterise the optimality of each condition at each point in the experiment, we first computed the ks-test statistic between the data (the average partial distance data) and the random order distribution and the first-two optimal distribution. Since smaller ks-statistics indicate a closer match between the distributions, we then took the ratio of the ks-statistics (random over first two-optimal). Values less than one indicate that the data are more consistent with random than optimal responding. Values greater than one indicate that the data are more consistent with optimal rather than random responding. 

```{r sampling_dist, echo=FALSE, warning=FALSE, message=FALSE, results="asis"}

opt_order_dist = function(trial_optimal_order, nperms=nperms, allperms=allperms){
  pdist <- rep(NA, nperms)
  for (i in 1:nperms){
    distObj = ConDisPairs(table(allperms[i, ], trial_optimal_order))
    pdist[i] = distObj$D  
  }
  return(pdist)
}

opt_order = c(1,2,3,4)
opt_dist_list = opt_order_dist(opt_order, nperms, allperms)

nsamples = 1000

# sample a bunch of orders, and compute the distances
allrand = as.data.frame(list("s" = sample(opt_dist_list, nsamples, replace = TRUE)))
onefix = as.data.frame(list("s" = sample(opt_dist_list[allperms[,1]==1], nsamples, replace = TRUE)))
twofix = as.data.frame(list("s" = sample(opt_dist_list[allperms[,1]==1 & allperms[,2]==2], nsamples, replace = TRUE)))
distance_samples = as.data.frame(rbind(cbind("cond" = rep(1, nsamples), "s" = allrand$s), cbind("cond" = rep(2, nsamples), "s" = onefix$s), cbind("cond" = rep(3, nsamples), "s" = twofix$s)))


# plot the distribution of distances
samp_plot <- distance_samples %>% ggplot(aes(s)) + 
  geom_histogram(color="white", alpha =0.8, bins=7, position="identity") + 
  labs(y = "Frequency of Sample", x = "Distance From Optimal") + 
  facet_wrap(~cond)
print(samp_plot)

# Statistical comparison
#ks.test(avg_avgdata$avg[avg_avgdata$condition == "fixed" & avg_avgdata$phase == "deadline early"], avg_avgdata$avg[avg_avgdata$condition == "fixed" & avg_avgdata$phase == "deadline late"])

# Lower values of the ks test indicate higher similarity between the distributions, so compare the ratio of optimal vs random distances at each time point
# Note: you don't have to sample for the kstest, you can just use the actual values, but sampling is fine
# > 1 indicates more optimal than random, < 1 indicates more random than optimal

ks.fixed.phase1 = ks.test(allrand$s, selections %>% filter(condition == "fixed", phase == "untimed") %>% pull(avgD))$statistic/ks.test(twofix$s, selections %>% filter(condition == "fixed", phase == "untimed") %>% pull(avgD))$statistic

ks.fixed.phase2 = ks.test(allrand$s, selections %>% filter(condition == "fixed", phase == "deadline") %>% pull(avgD))$statistic/ks.test(twofix$s, selections %>% filter(condition == "fixed", phase == "deadline") %>% pull(avgD))$statistic

ks.random.phase1 = ks.test(allrand$s, selections %>% filter(condition == "random", phase == "untimed") %>% pull(avgD))$statistic/ks.test(twofix$s, selections %>% filter(condition == "random", phase == "untimed") %>% pull(avgD))$statistic

ks.random.phase2 = ks.test(allrand$s, selections %>% filter(condition == "random", phase == "deadline") %>% pull(avgD))$statistic/ks.test(twofix$s, selections %>% filter(condition == "random", phase == "deadline") %>% pull(avgD))$statistic

# Build data for plotting
randomness = as.data.frame(list("condition" = factor(c("fixed", "fixed", "random", "random"), levels =c("fixed", "random")), 
                                "phase" = factor(c("untimed", "deadline", "untimed", "deadline"), levels = c("untimed", "deadline")), 
                                "ratio" = c(ks.fixed.phase1, ks.fixed.phase2, ks.random.phase1, ks.random.phase2)))
levels(randomness$phase) = c("untimed", "deadline")

# Plot
random_eval_plot <- randomness %>% ggplot(aes(x=phase, y=ratio, group=condition)) + 
                     geom_line(aes(linetype=condition), size=1) +
                     geom_point(size=4) + 
                     labs(y="Ratio (Random Distance/Optimal Distance)", x = "Phase") + 
                     ggtitle("Comparison to random and optimal sampling distributions") + 
                     geom_hline(yintercept =1, linetype="dotted", color="black", size=.5)
print(random_eval_plot)

```

This figure efficiently summarises the main result: responding is more optimal in the fixed deadline condition particularly during the last ten trials; in the random deadline conditions, responding was closer to a random sampling distribution than to an optimal sampling distribution. 

## Selection Choice RTs

```{r first_choice_rt, echo=FALSE, warning=FALSE, message=FALSE, results="asis"}
# selection_mean_rts = selections %>% filter(rt_1 < 3000 | rt_2 < 3000 | rt_3 < 3000 | rt_4 < 3000) %>% group_by(condition, phase) %>% summarise("mrt_sel1" = round(mean(rt_1, na.rm=TRUE)), "mrt_sel2" = round(mean(rt_2, na.rm=TRUE)), "mrt_sel3" = round(mean(rt_3, na.rm=TRUE)), "mrt_sel4" = round(mean(rt_4, na.rm=TRUE))) 
# 
# selection_sd_rts = selections %>% filter(rt_1 < 3000 | rt_2 < 3000 | rt_3 < 3000 | rt_4 < 3000) %>% group_by(condition, phase) %>% summarise("sd_sel1" = round(sd(rt_1, na.rm=TRUE)), "sd_sel2" = round(sd(rt_2, na.rm=TRUE)), "sd_sel3" = round(sd(rt_3, na.rm=TRUE)), "sd_sel4" = round(sd(rt_4, na.rm=TRUE))) 

selection_mean_rts = selections %>% group_by(condition, phase) %>% summarise("mrt_sel1" = round(mean(rt_1, na.rm=TRUE)), "mrt_sel2" = round(mean(rt_2, na.rm=TRUE)), "mrt_sel3" = round(mean(rt_3, na.rm=TRUE)), "mrt_sel4" = round(mean(rt_4, na.rm=TRUE))) 
 
selection_sd_rts = selections %>% filter(rt_1 < 3000 | rt_2 < 3000 | rt_3 < 3000 | rt_4 < 3000) %>% group_by(condition, phase) %>% summarise("sd_sel1" = round(sd(rt_1, na.rm=TRUE))/sqrt(nrow(selections)), "sd_sel2" = round(sd(rt_2, na.rm=TRUE))/sqrt(nrow(selections)), "sd_sel3" = round(sd(rt_3, na.rm=TRUE))/sqrt(nrow(selections)), "sd_sel4" = round(sd(rt_4, na.rm=TRUE))/sqrt(nrow(selections))) 

kable(selection_mean_rts, caption = "Mean RTs for each scheduling selection")
kable(selection_sd_rts, caption = "SD RTs for each scheduling selection")

```


```{r detectMissingSelectionData, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
# Cycle through every trial, from every subject, from each phase, of every condition 
# Determine if the recorded RDK data matches the selection data
conditions = c("fixed", "random")
phases = c("untimed", "deadline")
max_trials = c(9, 29) # number of trials in the untimed and deadline conditions
# finalSubjects - list of final subject numbers (uniqueIDs)


udata <- data %>% filter(phase == "untimed")
ddata <- data %>% filter(phase == "deadline")


troubleMakers = setNames(data.frame(matrix(ncol = 4, nrow = 0)), c("condition", "uniqueid", "phase", "trial_number"))
rcnt= 1;

cnt = 1
total = sum(final_n$n) * 10
pb <- txtProgressBar(min = 0, max = total, style = 3)
# Cycle through untimed data first
for (i in 1:sum(final_n$n)){
  # Extract subject data
  sdata <- udata %>% filter(uniqueid == finalSubjects$uniqueid[i])
  
  for (j in 0:max_trials[1]){
    # Extract trial data
    tdata <- sdata %>% filter(trial_number == j)
    
    nSelectionEvents = tdata %>% filter(trial_event == "practice_rdk") %>% count()
    nRdkEvents = tdata %>% filter(trial_event == "rdk") %>% select(dot_coherence) %>% unique() %>% count()
    
    if (nRdkEvents <= nSelectionEvents){ # do nothing
    } else {
      # Add current subject, condition, phase, and trial to the list of troublesome trials      
      troubleMakers[rcnt, ] = c(tdata$condition[1], tdata$uniqueid[1], tdata$phase[1], tdata$trial_number[1])
      rcnt = rcnt + 1
    }
  
  cnt = cnt + 1
  setTxtProgressBar(pb, cnt)
  }
}


cnt = 1
total = sum(final_n$n) * 30
pb <- txtProgressBar(min = 0, max = total, style = 3)
# Cycle through untimed data first
for (i in 1:sum(final_n$n)){
  # Extract subject data
  sdata <- ddata %>% filter(uniqueid == finalSubjects$uniqueid[i])
  
  for (j in 0:max_trials[2]){
    # Extract trial data
    tdata <- sdata %>% filter(trial_number == j)
    
    nSelectionEvents = tdata %>% filter(trial_event == "select_rdk") %>% count()
    nRdkEvents = tdata %>% filter(trial_event == "rdk") %>% select(dot_coherence) %>% unique() %>% count()
    
    if (nRdkEvents <= nSelectionEvents){ # do nothing
    } else {
      # Add current subject, condition, phase, and trial to the list of troublesome trials      
      troubleMakers[rcnt, ] = c(tdata$condition[1], tdata$uniqueid[1], tdata$phase[1], tdata$trial_number[1])
      rcnt = rcnt + 1
    }
  
  cnt = cnt + 1
  setTxtProgressBar(pb, cnt)
  }
}

```
