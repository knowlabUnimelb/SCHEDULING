---
title: "Experiment 4: RDK Direction Judgement, 8 Tasks, 500 msec Error Penalty"
author: "knowlabUnimelb"
date: "2020-10-29"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

Daniel R. Little^1^ and Ami Eidels^2^
^1^ The University of Melbourne, ^2^ The University of Newcastle

```{r TODO, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
# getRDKselections not set up for 8 tasks
# TODO: set up loglinear model
```

```{r load_modules, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
library(tidyverse)
library(workflowr)
#library(Rankcluster)
#library(rankdist) # Crashes rStudio
#library(Kendall)
library(DescTools) # ConDisPairs looks useful
library(gtools) 
library(english)
library(ggplot2)
library(dplyr)
#library(hrbrthemes)
library(knitr)
library(reshape2)
library(png)
library(grid)
library(lme4)
library(lmerTest)
library(rstatix)
library(jpeg)
library(pmr)
library(jmv)
library(betareg)
library(statmod)
```

```{r load_data, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
rm(list=ls())

# Load scheduling analysis functions
#  This file contains functions which are useful across all scheduling experiments
source(paste("analysis", "scheduling_analysis_functions.R", sep="/"))
#source(paste("analysis", "additional_scripts.R", sep="/"))

# Get date and format to YYMMDD
date <- getTodaysDate()

# Set the name of the experiment
experimentName = "exp4_rdk_8tasks" # use the index.Rmd file to track which experiment is which

# Name of the data file containing raw data
datafilename = "2022_exp4_rdk_data_8tasks.csv" # Name of data file
inputdir <- "data" # location of raw data
datafn  = paste(inputdir, datafilename,sep="/") # Full data filename

# Read data dictionary to get column names
dataDictionaryfn = "data_dictionary_8.csv";
colfile = read.csv(paste(inputdir, dataDictionaryfn, sep="/"), stringsAsFactors = FALSE) # specifies the format of the columns

# Location to save selection file
selectiondir <- "selections"
selectionfn = paste(str_split(datafilename, '.csv', simplify=TRUE)[1], '_selections.csv', sep='');
selectionOutputFile <- paste(inputdir, selectiondir, selectionfn, sep="/")

# Location to save stats analysis files
outputfolder = paste(".", "analysis", "anovaData", sep = '/')  # Output folder

# Read the rawdata
rawdata = read.csv(datafn, header = FALSE, col.names = colfile$Column, colClasses = colfile$Type, stringsAsFactors = FALSE, na.strings='NA') # Add column labels to data

# Remove handful of subjects who partially completed the experiment without an id, 
rawdata = rawdata[!is.na(rawdata$unique_id), ]

# Summary data
loggedSubjects = rawdata %>% distinct(condition, unique_id)
nLoggedSubjects = rawdata %>% distinct(condition, unique_id) %>% count(condition)

```

```{r data_cleaning, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
dataCleaningOutput = cleanRDKdata(rawdata)
cleandata <- dataCleaningOutput$cleandata
subjects <- dataCleaningOutput$subjects
nSubjects <- dataCleaningOutput$nSubjects

# Note:
# phase labels are:
# displayed_length_list (this is the exploratory phase where they click on projects)
# practice_selection - selection phase; practice - typing the selected list
# long_deadline_selection - long deadline phase (x 10); untimed - typing the list
# short_deadline_selection (x 30); deadline - typing the list
```

```{r add_phase_col, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
  cleandata = add_phase_col(cleandata, subjects$unique_id)
  cleandata$phase = as.factor(cleandata$phase)
  levels(cleandata$phase) = c("untimed", "deadline")
```

```{r demographics, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
# Demographics
demographics = getDemographics(rawdata, nSubjects, subjects)
```

# Method

## Participants

## Design

```{r getCoherenceSet, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
# Get all coherence options
coherence_set = sort(unique(c(cleandata$patch_0, cleandata$patch_1, cleandata$patch_2, cleandata$patch_3, cleandata$patch_4, cleandata$patch_5, cleandata$patch_6, cleandata$patch_7)), decreasing = TRUE)

```

```{r taskImage, echo=FALSE, warning=FALSE, message=FALSE, results="asis"}
img <- readPNG("analysis/patch_selection_8.png")
grid.raster(img)
```

_Data Cleaning_

```{r identify_nonlearners, echo=FALSE, warning=FALSE, message=FALSE, results="asis"}
rdkoutput = removeRDKnonlearners(cleandata, dot_coherence, FALSE)
data = rdkoutput$data
rdk  = rdkoutput$rdk 
finalSubjects = rdkoutput$finalSubjects 
final_n = rdkoutput$final_n
avgTimeoutPerSubject=rdkoutput$avgTimeoutPerSubject
```

# Data Analysis

We first summarize performance by answering the following questions: 

## Task completions

* How many tasks are completed on average?

```{r task_completion, echo=FALSE, message=FALSE, warning=FALSE, results="asis"}
taskStatsOutput = analyseRDKTaskCompletions_phaseOnly(rdk, experimentName, outputfolder, printOutput = FALSE)
avgCompletions = taskStatsOutput$avgCompletions

# Uncomment to see task completion table
print(taskStatsOutput$table) 

# Uncomment to see stats output
print(taskStatsOutput$model) 

```

## RDK performance

```{r rdk_anova, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
rdkANOVAoutput = analyseRDKdata_phaseOnly(rdk, coherence_set, levels(cleandata$phase))
rdkANOVAdata = rdkANOVAoutput$anovaData
rdkANOVAaccuracy = rdkANOVAoutput$accuracy
rdkANOVAsummedRT = rdkANOVAoutput$summedRT
```

* What was the average completion time and accuracy of the easy, medium, hard, and very hard tasks? 

```{r difficulty_plot, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
makeRDKplots_phaseOnly(rdkANOVAdata, coherence_set)
```

We further broke down RTs by condition, deadline, and difficulty. 

```{r conXphaseXdiff, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
diffOutput = computeRewardRate_phaseOnly(rdk)
kable(diffOutput$difficulty, digits = 2, caption="Mean accuracy, RT, summed RT, and reward rate attempts for each difficulty and each phase")

rrdata = diffOutput$rrdata

```

## Reward Rate

```{r rewardRate_analysis, echo=FALSE, warning=FALSE, message=FALSE, results="asis"}
# Plot reward rate
plotRewardRate_phaseOnly(rrdata)

# Reward Rate ANOVA
rrAOV = rewardRateANOVA_phaseOnly(rrdata);
rr.aov = rrAOV$anovaData

# Within subjects
kable(rr.aov$rmTable$asDF, digits=3)

# Between subjects
kable(rr.aov$bsTable$asDF, digits=3)

```

## Optimality in each condition

Having now established that the RDK's are ordered in accuracy, difficulty, and reward rate, it is clear that the task set presented to each subject has an optimal solution, ordered from easiest to most difficult. We now ask: 

* What is the proportion of easy, medium, hard, and very hard tasks selected first, second, third or fourth? 

```{r get_selections, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
numberOfTasks = 8 # Number of tasks

if (!file.exists(selectionOutputFile)){
  selection_data = data %>% filter(trial_event %in% c("practice_rdk", "select_rdk"))
  selection_data$trial_event = droplevels(selection_data$trial_event) # drop demographics level
  
  ph = c(rep("untimed", 10), rep("deadline", 30))
  cn = c(rep("practice_rdk", 10), rep("select_rdk", 30))
  tr = c(seq(0,9), seq(0,29))
  sdLength = length(cn) # selection data length for each subject
  
  # Convert location selections to difficulty order selections
  selections = data.frame(subject = rep(finalSubjects$unique_id, each = sdLength), condition = rep(finalSubjects$condition, each = sdLength), phase = rep(ph, sum(final_n$n)), trial = rep(tr, sum(final_n$n)), selected_difficulty_1 = rep(NA, sum(final_n$n) * sdLength), selected_difficulty_2 = rep(NA, sum(final_n$n) * sdLength), selected_difficulty_3 = rep(NA, sum(final_n$n) * sdLength), selected_difficulty_4 = rep(NA, sum(final_n$n) * sdLength), selected_difficulty_5 = rep(NA, sum(final_n$n) * sdLength), selected_difficulty_6 = rep(NA, sum(final_n$n) * sdLength), selected_difficulty_7 = rep(NA, sum(final_n$n) * sdLength), selected_difficulty_8 = rep(NA, sum(final_n$n) * sdLength),                        rt_1 = rep(NA, sum(final_n$n) * sdLength), rt_2 = rep(NA, sum(final_n$n) * sdLength), rt_3 = rep(NA, sum(final_n$n) * sdLength), rt_4 = rep(NA, sum(final_n$n) * sdLength), rt_5 = rep(NA, sum(final_n$n) * sdLength), rt_6 = rep(NA, sum(final_n$n) * sdLength), rt_7 = rep(NA, sum(final_n$n) * sdLength), rt_8 = rep(NA, sum(final_n$n) * sdLength))
  selections$phase <- factor(selections$phase, levels = c("untimed", "deadline"))
  
  
  cnt = 1
  total = sum(final_n$n) * sdLength
  pb <- txtProgressBar(min = 0, max = total, style = 3)
  for (i in 1:sum(final_n$n)){
    for (j in 1:sdLength){
      # Coherence in each location (W, N, E, S) 
      coherence_locations = selection_data %>% filter(unique_id == finalSubjects$unique_id[i], trial_event == cn[j], trial_number == tr[j]) %>% slice(1) %>% select(patch_0, patch_1, patch_2, patch_3, patch_4, patch_5, patch_6, patch_7)
      location_selections = selection_data %>% filter(unique_id == finalSubjects$unique_id[i], trial_event == cn[j], trial_number == tr[j]) %>% pull(button_pressed) + 1
      
      coherence_selections = as.numeric(as.vector(coherence_locations)[location_selections[!is.na(location_selections)]])
      length(coherence_selections) <- numberOfTasks # pad end with NAs
      
      selection_rt = selection_data %>% filter(unique_id == finalSubjects$unique_id[i], trial_event == cn[j], trial_number == tr[j]) %>% pull(rt)
      length(selection_rt) <- numberOfTasks # pad end with NAs
      
      selections$selected_difficulty_1[cnt] = c(which(coherence_set == coherence_selections[1]), NA)[1]
      selections$selected_difficulty_2[cnt] = c(which(coherence_set == coherence_selections[2]), NA)[1]
      selections$selected_difficulty_3[cnt] = c(which(coherence_set == coherence_selections[3]), NA)[1]
      selections$selected_difficulty_4[cnt] = c(which(coherence_set == coherence_selections[4]), NA)[1]
      selections$selected_difficulty_5[cnt] = c(which(coherence_set == coherence_selections[5]), NA)[1]
      selections$selected_difficulty_6[cnt] = c(which(coherence_set == coherence_selections[6]), NA)[1]
      selections$selected_difficulty_7[cnt] = c(which(coherence_set == coherence_selections[7]), NA)[1]
      selections$selected_difficulty_8[cnt] = c(which(coherence_set == coherence_selections[8]), NA)[1]
      
      selections$rt_1[cnt] = selection_rt[1]
      selections$rt_2[cnt] = selection_rt[2]
      selections$rt_3[cnt] = selection_rt[3]
      selections$rt_4[cnt] = selection_rt[4]
      selections$rt_5[cnt] = selection_rt[5]
      selections$rt_6[cnt] = selection_rt[6]
      selections$rt_7[cnt] = selection_rt[7]
      selections$rt_8[cnt] = selection_rt[8]
      
      cnt = cnt + 1
      setTxtProgressBar(pb, cnt)
    }
  }
  close(pb)
  write.csv(selections, selectionOutputFile)
} else {
  
  selections = read.csv(selectionOutputFile)
  subset(selections, select = -c(X)) # Drop row number column called X
}
```

```{r find_heatmap, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
fixed_untimed   = selections %>% filter(phase == "untimed") %>% select(selected_difficulty_1, selected_difficulty_2, selected_difficulty_3, selected_difficulty_4, selected_difficulty_5, selected_difficulty_6, selected_difficulty_7, selected_difficulty_8) %>% as.matrix()
fixed_deadline  = selections %>% filter(phase == "deadline") %>% select(selected_difficulty_1, selected_difficulty_2, selected_difficulty_3, selected_difficulty_4, selected_difficulty_5, selected_difficulty_6, selected_difficulty_7, selected_difficulty_8) %>% as.matrix()


imputeMissingData = function(data, n = numberOfTasks){
  for (i in 1:n){
      data[is.na(data[,i]), i] = mean(data[,i], na.rm=TRUE)
  }
  return(data)
}

#fixed_untimed = imputeMissingData(fixed_untimed, numberOfTasks)
#fixed_deadline = imputeMissingData(fixed_deadline, numberOfTasks)

# rankagg is slow with 8 options
#fu = fixed_untimed %>% rankagg() %>% destat()
fu = as.matrix(as_tibble(fixed_untimed) %>% drop_na() %>% group_by_all() %>% count()) %>% destat()
#fd = fixed_deadline %>% rankagg() %>% destat()
fd = as.matrix(as_tibble(fixed_deadline) %>% drop_na() %>% group_by_all() %>% count()) %>% destat()

```

```{r heatmap_plot_over_subs, echo=FALSE, warning=FALSE, message=FALSE, results="asis"}
  plotHeatmap_phaseOnly(fu, fd, c('fixed'))
```

* Do the marginal distributions differ from uniformity?

We tested whether the marginal distributions were different from uniformally random selection using the fact that the mean rank is distributed according to a  $\chi^2$ distribution with the following test-statistic:
$$\chi^2 = \frac{12N}{k(k+1)}\sum_{j=1}^k \left(m_j - \frac{k+1}{2} \right)^2$$
see (Marden, 1995). 

```{r chi2uniformity, echo=FALSE, warning=FALSE, message=FALSE, results="asis"}
selection_n = selections %>% count( phase) 

# Compare heatmaps to uniform
chi2UniformRank = function(observed, n, k = numberOfTasks){
  ex <- rep((k + 1)/2, k);
  oc <- observed
  
  chi = (12 * n/(k * (k + 1))) * sum((oc - ex)^2) # Test statistic of mean rank under uniformity
  p = dchisq(chi, k-1)
  return(list("chi2" = chi, "chi2p" = p))
}

fu$uni <- chi2UniformRank(fu$mean.rank, selection_n$n[selection_n$phase == "untimed"])
fd$uni <- chi2UniformRank(fd$mean.rank, selection_n$n[selection_n$phase == "deadline"])

chiUniformTable = data.frame("condition" = c("fixed", "fixed"), 
                            "phase" = c("untimed", "deadline"), 
                            "chi2" = round(c(fu$uni$chi2, fd$uni$chi2),2), 
                            "df" = c(3,3),
                            "p" = round(c(fu$uni$chi2p, fd$uni$chi2p), 2))

knitr::kable(chiUniformTable, caption = "Chi2 test of uniformity")

```

```{r hierarchical_loglinear_model , echo=FALSE, warning=FALSE, message=FALSE, results="asis"}
# TODO: set up loglinear model

# hmOutput = heatmapStats(selections)

# Uncomment to see output
# summary(hmOutput)
```

```{r distanceAnalysis, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
# Add distances to selections data frame
selections = getDistances_8(selections)
plotDistances_phaseOnly(selections, 'Avg Subject Distance')

numberOfTasks = 8
maxdistance = (numberOfTasks * (numberOfTasks-1))/2

distanceAnalysis = distanceStats_phaseOnly(selections, maxdistance)
summary(distanceAnalysis)

#ksOutput = analyseDistances(selections, c("fixed_location_delay", "random_location_delay"), c("untimed", "deadline"))

# Uncomment to view output
# print(ksOutput)

```

## Stability of selections
```{r entropyAnalysis, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
subEntropy = getSubjectEntropy_8(selections)
makeSubjectEntropyPlot_phaseOnly(subEntropy, 'subject entropy')
entropyModel = entropyStats_phaseOnly(subEntropy, 20)

```

```{r entropyBlockAnalysis, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
blockEntropy = getEntropyOverBlocks_8(selections)
makeBlockEntropyPlot_phaseOnly(blockEntropy, 'entropy over blocks')

subBlockEntropy = getSubjectEntropyOverBlocks_8(selections)
# blockEntropyModel = lmer(mean_entropy ~ condition + block + condition:block + (1|subject), subBlockEntropy)
blockEntropyModel = lmer(mean_entropy ~ block + (1|subject), subBlockEntropy)
summary(blockEntropyModel)
```

## Selection Choice RTs

```{r first_choice_rt, echo=FALSE, warning=FALSE, message=FALSE, results="asis"}
# selection_mean_rts = selections %>% 
#   group_by(condition, phase) %>% 
#   summarise("mrt_sel1" = mean(rt_1, na.rm=TRUE), "mrt_sel2" = mean(rt_2, na.rm=TRUE), "mrt_sel3" = mean(rt_3, na.rm=TRUE), "mrt_sel4" = mean(rt_4, na.rm=TRUE)) 

selection_mean_rts <- selections %>%
  group_by(condition, phase) %>%
  summarise(across(starts_with("rt_"), ~mean(.x, na.rm = TRUE), .names = "mrt_sel{.col}"))

kable(selection_mean_rts, caption = "Mean RTs for each scheduling selection", digits = 2)

rtAOV = selectionRTstats_phaseOnly(selections)
print(rtAOV)
```

# Selection model

We can treat each task selection as a probabilistic choice given by a Luce's choice rule (Luce, 1959), where each task is represented by some strength, $\nu$. The probability of selecting task $i_j$ from set $S = \left{i_1, i_2, ..., i_J \right}$, where J is the number of tasks, is:

$$p\left(i_j |S \right) = \frac{\nu_{i_j}}{\sum_{i \in S} \nu_{i}} $$.

Plackett (1975) generalised this model to explain the distribution over a sequence of choices (i.e., ranks). In this case, after each choice, the choice set is reduce by one (i.e., sampling without replacement). This probability of observing a specific selection order, $i_1 \succ ... \succ i_J$ is:

$$p\left(i_j |A \right) = \prod_{j=1}^J \frac{\nu_{i_j}}{\sum_{i \in A_j} \nu_{i}} $$, 

where $A_j$ is the current choice set.


```{r PLmodel, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}

# # Estimate Plackett-Luce model parameters
# # THis code fails
# #fu.pl = fixed_untimed %>% rankagg() %>% pl() # use fd.pl@details to access results
# #fd.pl = fixed_deadline %>% rankagg() %>% pl()
# 
# 
# # Unclear if this will work with size 8 because the code looks very inefficient
# # PL analysis code that words but is slow
# fu.pl = as.matrix(as_tibble(fixed_untimed) %>% drop_na() %>% group_by_all() %>% count()) %>% pl() # use fd.pl@details to access results
# fd.pl = as.matrix(as_tibble(fixed_deadline) %>% drop_na() %>% group_by_all() %>% count())  %>% pl()
# 
# # Output of slowPL analysis
# #fu.pl@coef = c(1.6607055, 1.1725421, 1.1120497, 1.1174395, 0.7435779, 0.8604619, 0.7332341, 0.5988375)
# #fd.pl@coef = c(1.8100339, 1.2227968, 1.2912747, 1.1518640, 0.6417747, 0.8350447, 0.7021752, 0.3433166)
# 
# slopeLP = function(coef){
#     # Fit a line to the coefficients
#     df = data.frame('diff' = seq(4,1), 'x' = coef)
#     lmod = lm(x ~ diff, df)
#     slope = round(lmod$coefficients['diff'], 4)
#     return("slope" = slope)
# }
# 
# PlackettLuceParms = data.frame("condition" = c("fixed", "fixed"), 
#                               "phase" = c("untimed", "deadline"), 
#                               "1" = round(c(fu.pl@coef[1], fd.pl@coef[1]),2),
#                               "2" = round(c(fu.pl@coef[2], fd.pl@coef[2]),2),
#                               "3" = round(c(fu.pl@coef[3], fd.pl@coef[3]),2),
#                               "4" = round(c(fu.pl@coef[4], fd.pl@coef[4]),2), 
#                               "5" = round(c(fu.pl@coef[5], fd.pl@coef[5]),2),
#                               "6" = round(c(fu.pl@coef[6], fd.pl@coef[6]),2),
#                               "7" = round(c(fu.pl@coef[7], fd.pl@coef[7]),2),
#                               "8" = round(c(fu.pl@coef[8], fd.pl@coef[8]),2),                               
#                               "Slope" = c(slopeLP(fu.pl@coef), slopeLP(fd.pl@coef)))
# knitr::kable(PlackettLuceParms, caption = "Plackett-Luce strength parameter estimates")
# 
# 
# plSample = function(coefs){
#   coefs[coefs<0] <- 0
#   J = length(coefs)
#   A = 1:J
#   v = coefs/sum(coefs)
#   s = rep(NA, J)
#   for (i in 1:(J-1)){
#     s[i] = sample(A, 1, prob=v)
#     v = v[A!=s[i]]
#     v = v/sum(v)
#     A = A[A!=s[i]]
#   }
#   s[J] = A
#   return(s)
# }
# 
# plSamples = function(coefs, n){
#     s = matrix(NA, n, 8)
#     for (i in 1:n){
#         s[i,] = as.matrix(plSample(coefs))
#     }
#     return(s)
# }
# 
# # # TODO: Sample selection orders using pl coefficients
#  nsim = 100
#  sim_pl = data.frame("condition" = c(rep("fixed", nsim*2)),
#                      "phase" = c(rep("untimed", nsim), rep("deadline", nsim)),
#                      "sim" = rbind(plSamples(fu.pl@coef, nsim), plSamples(fd.pl@coef, nsim)))
#  sim_pl$phase <- factor(sim_pl$phase, levels=c('untimed', 'deadline'))
#  
#  # Add average distance column
#  sim_pl = sim_pl %>% mutate("avgD" = apply(sim_pl %>% select(sim.1, sim.2, sim.3, sim.4, sim.5, sim.6, sim.7, sim.8), 1, function(x)(scorePartialMatches(x, allperms, pdist)$avgD)))
#  
#  #       Compare resulting distribution with data
#  sim_avg_plot <- sim_pl %>% ggplot(aes(x=avgD, fill=phase)) + geom_histogram(aes(y=..density..), color="white", alpha=0.6, bins = 6, position="identity") + facet_wrap(~condition+phase) + labs(y="Probability", x = "Avg Distance")
#  print(sim_avg_plot + ggtitle("Avg Distance - Plackett-Luce Model"))
```

```{r notes, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
#files_to_build <- list.files("analysis", pattern = "^analysis_exp4_.*\\.Rmd$", full.names = TRUE)
#wflow_build(files = files_to_build)
#wflow_publish(files_to_build, message = "Update analysis code to use shared file repo")
#wflow_git_push() - returns an error: Error: Push failed for unknown reason.
#  Try opening a terminal, navigate to C:\Users\littled\Dropbox\Work\SCHEDULING
#  git push -u origin master
```
